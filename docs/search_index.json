[
["index.html", "Einführung in die Statistik Vorwort", " Einführung in die Statistik Tobias Krueger 2020-11-16 Vorwort Dies ist das Skript für den Kurs ‘Einführung in die Statistik’ am Geographischen Institut der Humboldt-Universität zu Berlin. Danksagung: Besonderer Dank gebührt Kassandra Jensch und Maeve Smyth für die Unterstützung bei der Erstellung dieses Skriptes. "],
["01-einfuehrung.html", "Kapitel 1 Einführung 1.1 Statistik im empirischen Forschungsprozess 1.2 Warum Statistik? 1.3 Organisatorisches 1.4 Mathematische Notation und Grundlagen 1.5 Quiz", " Kapitel 1 Einführung Das Modul B3 Einführung in die Statistik und das Fach Geographie besteht aus: Dem Seminar Einführung in die Statistik mit Tobias Krüger (regelmäßiges ZOOM-Treffen, montags 11:00 - 12:30, s. Moodle) Dem Seminar Einführung in die Geographie mit Christoph Schneider und Henning Nuissl Der PC-Übung Statistische Datenverarbeitung mit Matthias Baumann, Sebastian Schubert, Heidi Kreibich, Abror Gafurov, Hoseung Jung, Friedrich Busch, Kassandra Jensch und Maeve Smyth (ab 27. November, freitags 09:00 - 12:00, dazu mehr weiter unten) Das vorliegende Skript ist Grundlage des Seminars Einführung in die Statistik und dient der theoretischen Vorbereitung der PC-Übung Statistische Datenverarbeitung. Organisatorisches dazu weiter unten. Am Ende des Semesters haben Sie ein Grundverständnis der beschreibenden und der schließenden Statistik erworben. Sie können folgende Methoden selbständig in der Software R anwenden und deren Ergebnisse interpretieren: Beschreibung von Stichproben mit Zentralitäts- und Streuungsmaßen Beschreibung der Korrelation zwischen Merkmalen Schätzen von Verteilungsparametern anhand von Stichproben Test auf Unterschiede in Mittelwerten (t-Test) Test auf Unterschiede in Varianzen (F-Test) Test auf Unterschiede in Verteilungen (Kolmogorov-Smirnov-Test) Test auf Unabhängigkeit (Chi-Quadrat-Test) Modellierung eines linearen Zusammenhangs zwischen Merkmalen (lineare Regression) Graphische Methoden wie Histogramm, Boxplot, Streudiagramm und Quantil-Quantil-Plot Zunächst aber ein paar einleitende Worte zur Statistik in der Geographie. 1.1 Statistik im empirischen Forschungsprozess Lesen Sie dazu bitte Kapitel 2.4 von Zimmermann-Janschitz (2014), s. Moodle. Gemäß der dort gewählten Kategorisierung befasst sich die Statistik hauptsächlich mit Datenanalyse (Punkt 7), obwohl die angrenzenden Schritte ebenfalls wichtig sind. Auf Auswahl der Untersuchungseinheit (Punkt 4), Datenerhebung (Punkt 5) und Datenaufbereitung (Punkt 6) werden wir in Kapitel 2 näher eingehen. Punkt 8 (Interpretation und Rückschlüsse) wird durchgehend eine Rolle spielen. 1.2 Warum Statistik? Statistik ist Teil des physisch- und humangeographischen Methodenpakets. Da die Erkenntnisse der Geographie in vielen Teilen auf dem empirischen Forschungsprozess basieren ist statistische Analyse als Argumentationsunterstützung und als Beweissicherung unumgänglich. Außerdem dient sie der Bestätigung oder Widerlegung theoretischer Ansätze und der Generierung neuer Informationen aus verfügbaren Daten. In der Praxis dient Statistik häufig als Entscheidungsgrundlage. Lesen Sie bitte Kapitel 2.3 (Zimmermann-Janschitz 2014) für konkrete Anwendungsbeispiele der Statistik in der Geographie. Am Ende jenes Kapitels finden Sie außerdem eine Erklärung der Teilbereiche der Statistik. Mit der deskriptiven (beschreibenden) Statistik beschäftigen wir uns in den Wochen 3 - 5. Dazu ist das Lehrbuch von Zimmermann-Janschitz (2014) wichtig. Mit der induktiven (schließenden) Statistik beschäfigen wir uns in den Wochen 8 - 13. Die Brücke zwischen diesen beiden Teilbereichen - wie es Zimmermann-Janschitz (2014) darstellt - ist die Wahrscheinlichkeitstheorie, mit deren Grundlagen wir uns in den Wochen 6 - 7 auseinandersetzen. 1.3 Organisatorisches Die Abfolge der Inhalte des Seminars und der PC-Übung finden Sie in Tabelle 1.1. Tabelle 1.1: Inhalte von Einführung in die Statistik und Statistische Datenverarbeitung. Der gelesene Stoff wird am darauffolgenden Montag im Seminar diskutiert. Die PC-Übung beginnt in Woche 4, am 27. November. Woche Lesen Diskutieren (Montagsseminar) PC-Übung (Freitag) 1 Warum Statistik?; Organisatorisches; Mathematische Grundlagen - - 2 Datenerhebung; Grundbegriffe; Skalen Warum Statistik?; Organisatorisches; Mathematische Grundlagen - 3 Deskriptive Statistik 1 Datenerhebung; Grundbegriffe; Skalen - 4 Deskriptive Statistik 2 Deskriptive Statistik 1 Pre-Übung 5 Deskriptive Statistik 3 Deskriptive Statistik 2 Einführung in R 6 Grundlagen der Wahrscheinlichkeitsrechnung Deskriptive Statistik 3 Dateneingabe, Datenformate, Skalen 7 Verteilungen Grundlagen der Wahrscheinlichkeitsrechnung Deskriptive Statistik 8 Schätzen von Verteilungsparametern Verteilungen Korrelation 9 Statistische Tests 1 Schätzen von Verteilungsparametern Schätzen von Verteilungsparametern 1 “per Hand” in R 10 Statistische Tests 2 Statistische Tests 1 Schätzen von Verteilungsparametern 2 in R 11 Statistische Tests 3 Statistische Tests 2 F- und t-Test 12 Lineare Regression 1 Statistische Tests 3 Kolmogorow-Smirnow- und Chi-Quadrat-Test 13 Lineare Regression 2 Lineare Regression 1 Lineare Regression 1 “per Hand” in R 14 Klausurvorbereitung Lineare Regression 2 Lineare Regression 2 “per Hand” in R 15 Statistik hinterfragen Klausurvorbereitung Lineare Regression 3 in R Der prinzipielle Lernmodus in diesem digitalen Semester ist das Lesen von Kapiteln aus dem vorliegenden Skript und aus zwei Lehrbüchern (Zimmermann-Janschitz 2014; Mittag 2016). Dazu gibt es jede Woche ein kleines Quiz auf Moodle. Diese werden automatisch bewertet und sind Voraussetzung für den Erhalt der 3 Leistungspunkte des Seminars Einführung in die Statistik. Im Seminar jeden Montag besteht die Möglichkeit, Fragen zu klären und den Stoff der vorherigen Woche zu diskutieren. Das Seminar findet über ZOOM statt, s. permanenter Link auf Moodle. Wichtig: Fragen und Diskussionswünsche müssen bis Ende jeder Woche über das Moodleforum eingereicht werden. Manche Fragen werden sich bereits über das Forum klären lassen. Im Selbststudium - unterstützt durch das Seminar - erarbeiten Sie sich so die Theorie, welche dann in der PC-Übung mittels der Software R zur Anwendung kommt. Die PC-Übung findet ab 27. November immer freitags 09:00 - 12:00 statt, ebenfalls über ZOOM. Wir werden mehrere thematische ZOOM-Räume anbieten, in die Sie nach Bedarf gehen können (Fragen zu Hausaufgaben, Fragen zu R, Fragen zur Theorie, …). In diesem Zeitfenster bearbeiten Sie jede Woche einen Übungszettel. Die Lehrende stehen währenddessen in den ZOOM-Räumen für Fragen zur Verfügung. Ein Lösungszettel wird am Ende bereitgestellt. Abschließend gibt es eine neue Hausaufgabe bis zur nächsten Woche. Der Lösungszettel wird vor der nächsten Übung bereitgestellt. Außerhalb des Zeitfensters am Freitag beantworten wir Fragen über das Moodle Forum. In den Hausaufgaben wird in der Übung Erprobtes auf einen anderen Datensatz angewendet oder erweitert. Die Hausaufgaben sind Voraussetzung für den Erhalt der 3 Leistungspunkte der PC-Übung Statistische Datenverarbeitung. Die Abgabe erfolgt über einen Abgabelink auf Moodle bis zum nächsten Mittwoch 24:00. Das Abgabeformat (HTML via R Markdown) wird in der ersten PC-Übung ausführlich erklärt und eingeübt. Es beinhaltet den R-Code, einen kurzen Text zur Beantwortung der Fragestellung bzw. Interpretation der Ergebnisse, sowie relevante (!) Abbildungen, Tabellen und Kenngrößen. Am Donnerstag wird das Lösungsblatt bereit gestellt. Unklarheiten und Probleme werden während der nächsten Übung geklärt (eigener ZOOM-Raum für Fragen zur letzten Hausaufgabe). Ein kurzes Wort zu unseren Erwartungen: In diesem digitalen Semester wird selbständiges Lernen noch wichtiger sein als in Präsenzsemestern. Das Seminar entspricht mit 3 Leistungspunkten einem Aufwand von 90 Stunden, d.h. 6 Stunden pro Woche. Bei maximal 1.5 Stunden “Präsenz” via ZOOM verbleiben mindestens 4.5 Stunden pro Woche für das Selbststudium des Skriptes und der ausgewählten Lehrbuchkapitel. Die PC-Übung entspricht mit 3 Leistungspunkten ebenfalls 90 Stunden, d.h. 7.5 Stunden pro Woche ab Semesterwoche 4. Bei 3 Stunden “Präsenz” via ZOOM verbleiben weitere 4.5 Stunden pro Woche für Nachbereitung und Hausaufgaben. Insgesamt sollten Sie sich also auf 9 Stunden pro Woche selbstständiges Arbeiten für Statistik einstellen. Die Modulabschlussprüfung ist dieses Jahr eine sogenannte take-home Klausur, die Sie - voraussichtlich am 05.03.2021 - zu Hause in einem vorgegeben Zeitrahmen selbständig bearbeiten. Ein Nachholtermin findet voraussichtlich am 16.04.2021 statt. Die Anmeldung erfolgt gegen Semesterende über AGNES bei Frau Schwedler. Die take-home Klausur wird stark an die Übungen und Hausaufgaben angelehnt sein. Außerdem wird es ein paar Fragen über Moodle geben - ähnlich der regelmäßigen Quizze. Insgesamt werden sich \\(\\frac{5}{6}\\) der zu erreichenden Punktzahl auf Einführung in die Statistik beziehen und \\(\\frac{1}{6}\\) auf Einführung in die Geographie. 1.4 Mathematische Notation und Grundlagen In diesem Unterkapitel werden wichtige Begriffe eingeführt und wichtige mathematische Grundlagen aus der Schule wiederholt. Lesen Sie bitte dazu Kapitel 1.2 von Zimmermann-Janschitz (2014). Dort werden anhand des Beispieles der Kostenaufstellung für eine “Statistikexkursion” die Begriffe Variable, Index und Summe eingeführt. Variable wird synonym mit Merkmal verwendet. In den Zeilen der Tabelle 1.1 in Zimmermann-Janschitz (2014) stehen dann die einzelnen Merkmalswerte oder einfach nur Werte für die Untersuchungselemente (statistische Einheiten). Jede statistische Einheit ist gekennzeichnet durch einen eigenen Index. An dieser Stelle sei ergänzt, dass ein Index auch unterschiedliche Variablen bezeichnen kann, z.B. \\(x_1, x_2, \\ldots\\). Die Summe verschiedener Merkmalswerte wird wie folgt abgekürzt: \\[\\begin{equation} \\sum_{i=1}^{n}x_i=x_1+x_2+\\ldots+x_{n-1}+x_n \\tag{1.1} \\end{equation}\\] Das Summenzeichen \\(\\Sigma\\) symbolisiert die Anweisung, die Merkmalswerte \\(x_i\\) zu addieren, wobei der Index \\(i\\) von 1 bis zur Anzahl der Werte \\(n\\) läuft. Eine ähnliche verkürzte Schreibweise gibt es für das Produkt: \\[\\begin{equation} \\prod_{i=1}^{n}x_i=x_1 \\cdot x_2 \\cdot \\ldots \\cdot x_{n-1} \\cdot x_n \\tag{1.2} \\end{equation}\\] Hier gibt das Produktzeichen \\(\\Pi\\) die Anweisung, die Merkmalswerte \\(x_i\\) zu multiplizieren, wobei wiederum der Index \\(i\\) von 1 bis zur Anzahl der Werte \\(n\\) läuft. Manchmal wird das Multiplikationszeichen weggelassen und es findet sich nur ein kleiner Abstand zwischen den zu multiplizierenden Größen: \\[\\begin{equation} \\prod_{i=1}^{n}x_i=x_1 \\, x_2 \\, \\ldots \\, x_{n-1} \\, x_n \\tag{1.3} \\end{equation}\\] Diese Schreibweise, die man häufig aus Platzgründen findet, impliziert in jedem Fall eine Multiplikation. Zwei weitere Begriffe, die Zimmermann-Janschitz (2014) nicht einführt, sind für diese Lehrveranstaltung noch wichtig, Vektor und Matrix: In einem Reihenvektor sind Größen (z.B. Merkmalswerte) horizontal angeordnet: \\[\\mathbf{x} = \\begin{pmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n \\end{pmatrix}\\] In einem Spaltevektor sind die Größen vertikal angeordnet: \\[\\mathbf{x} = \\begin{pmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{pmatrix}\\] In einer Matrix sind Größen wie in einer Tabelle angeordnet, z.B. Merkmalswerte unterschiedlicher Variablen (Spalten): \\[\\mathbf{X} = \\begin{pmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,p}\\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,p} \\end{pmatrix}\\] Hier hat jeder Merkmalswert zwei Indizes, einen für die statistische Einheit (hier bis Anzahl \\(n\\)) und einen für die Variable (hier bis Anzahl \\(p\\)). Vektoren und Matrizen werden in der Regel fett gedruckt, wobei Vektoren \\(\\mathbf{x}\\) mit Kleinbuchstaben und Matrizen \\(\\mathbf{X}\\) mit Großbuchstaben bezeichnet werden. Die Rechenregeln für Vektoren und Matrizen sind in der linearen Algebra zusammengefasst. Wir werden daraus nur Auszüge in den letzten Semesterwochen verwenden. 1.4.1 Exponential- und Logarithmusfunktion Zwei mathematische Funktionen sind für diese Lehrveranstaltung besonders wichtig, die Exponential- und die Logarithmusfunktion. Die folgende Darstellung ist inspiriert von Gelman and Nolan (2002). Stellen Sie sich eine Amöbe vor, die sich innerhalb einer Stunde teilt (Abbildung 1.1). Diese zwei Amöben teilen sich jede in einer weiteren Stunde usw. Wie lautet die Gleichung für die Anzahl Amöben, \\(y\\), als Funktion der Zeit, \\(t\\) (in Stunden)? Abbildung 1.1: Sich teilende Amöbe. Quelle: http://www.gutenberg.org/files/18451/18451-h/images/illus002.jpg. Die Gleichung lautet: \\[\\begin{equation} y=2^t \\tag{1.4} \\end{equation}\\] Dies ist eine Exponentialfunktion mit Basis \\(2\\) und Exponent \\(t\\). Abbildung 1.2 zeigt zwei Plots dieser Funktion. (Den verwendeten R-Code werden sie im Verlauf der PC-Übung verstehen.) t &lt;- seq(1, 6) y &lt;- 2^t plot(t, y, pch = 19, type = &#39;b&#39;) plot(t, log(y), pch = 19, type = &#39;b&#39;) Abbildung 1.2: Links: Plot von Gleichung (1.4). Rechts: Plot von Gleichung (1.4) auf logarithmischer Skala. Die Umkehrfunktion der Exponentialfunktion ist die Logarithmusfunktion: \\[\\begin{equation} log(y)=log(2^t)=t \\cdot log(2) \\tag{1.5} \\end{equation}\\] Da der Logarithmus von \\(y\\) eine lineare Funktion von \\(t\\) ist (Gleichung (1.5)), zeigt die rechte Seite von Abbildung 1.2 (\\(y\\) auf logarithmischer Skala) eine gerade Linie. Übliche Basen der Logarithmusfunktion sind: \\[\\begin{equation} log_2\\left(2^t\\right)=lb\\left(2^t\\right)=t \\tag{1.6} \\end{equation}\\] Dies ist der sogenannte binäre Logarithmus (lb). \\[\\begin{equation} log_{10}\\left(10^t\\right)=lg\\left(10^t\\right)=t \\tag{1.7} \\end{equation}\\] Dies ist der sogenannte dekadische Logarithmus (lg). \\[\\begin{equation} log_e\\left(e^t\\right)=ln\\left(e^t\\right)=t \\tag{1.8} \\end{equation}\\] Dies ist der sogenannte natürliche Logarithmus (ln), wobei \\(e \\approx 2.7183\\) die Eulersche Zahl ist. Achtung! Programmiersprachen wie R nutzen oft eine andere Notation, der wir auch in diesem Kurs folgen: \\[\\begin{equation} ln()=log() \\tag{1.9} \\end{equation}\\] \\[\\begin{equation} e^t=\\exp(t) \\tag{1.10} \\end{equation}\\] Die Rechenregeln der Exponentialfunktion sind: \\[\\begin{equation} a^m \\cdot a^n=a^{m+n} \\tag{1.11} \\end{equation}\\] \\[\\begin{equation} a^n \\cdot b^n=(a \\cdot b)^n \\tag{1.12} \\end{equation}\\] \\[\\begin{equation} \\frac{a^m}{a^n}=a^{m-n} \\tag{1.13} \\end{equation}\\] \\[\\begin{equation} \\frac{a^n}{b^n}=\\left(\\frac{a}{b}\\right)^n \\tag{1.14} \\end{equation}\\] \\[\\begin{equation} \\left(a^m\\right)^n=a^{m \\cdot n} \\tag{1.15} \\end{equation}\\] Die Rechenregeln der Logarithmusfunktion sind: \\[\\begin{equation} log(u \\cdot v)=log(u)+log(v) \\tag{1.16} \\end{equation}\\] \\[\\begin{equation} log\\left(\\frac{u}{v}\\right)=log(u)-log(v) \\tag{1.17} \\end{equation}\\] \\[\\begin{equation} log\\left(u^r\\right)=r \\cdot log(u) \\tag{1.18} \\end{equation}\\] 1.4.2 Quadratische Funktion und Wurzelfunktion Abschließend sei noch die quadratische Funktion erwähnt (Abbildung 1.3, links): \\[\\begin{equation} f(x)=x^2 \\tag{1.19} \\end{equation}\\] Und ihre Umkehrfunktion, die Wurzelfunktion (Abbildung 1.3, rechts): \\[\\begin{equation} \\sqrt{x^2}=x \\tag{1.20} \\end{equation}\\] x &lt;- seq(0, 5, 0.1) plot(x, x^2, pch = 19, type = &#39;b&#39;) plot(x, sqrt(x^2), pch = 19, type = &#39;b&#39;) Abbildung 1.3: Links: Quadratische Funktion von \\(x\\). Rechts: Wurzelfunktion von \\(x^2\\), s. Gleichung (1.20). 1.5 Quiz Auf Moodle unter Woche 1 finden Sie ein kurzes Quiz zu diesem Kapitel des Skriptes. Bitte beantworten Sie die Fragen bis Ende der Woche. Dies werden wir jede Woche tun, als Lernstandskontrolle für Sie und als Überblick für uns über mögliche Verständnisprobleme. Literatur "],
["02-begriffe.html", "Kapitel 2 Grundbegriffe und Datenerhebung 2.1 Statistische Grundbegriffe 2.2 Datenerhebung 2.3 Skalenniveaus", " Kapitel 2 Grundbegriffe und Datenerhebung Lesen Sie hierzu bitte Kapitel 3.1.2 und 3.1.3 von Zimmermann-Janschitz (2014). Im Folgenden finden Sie Leitfragen und Ergänzungen zu diesen Kapiteln. 2.1 Statistische Grundbegriffe Die statistische Masse umfasst all jene Elemente (Anzahl \\(N\\)), die für eine statistische Untersuchung Relevanz besitzen. Für die Bestimmung der statistischen Masse sind inhaltliche, zeitliche und räumliche Abgrenzungskriterien erforderlich. Alternative Begriffe sind (die geläufigsten fett gedruckt): statistische Grundmenge, Grundgesamtheit, Population, Kollektiv. Beispiele finden Sie in Zimmermann-Janschitz (2014), Kapitel 3.1.3. Die statistische Einheit \\(e_i\\) mit \\(i=1, \\ldots, n\\) stellt das Untersuchungselement und somit die kleinste, nicht weiter unterteilbare Einheit in einer statistischen Untersuchung dar. Diese statistische Einheit trägt jene Information (auch als Merkmal \\(X\\) bezeichnet), die im Zentrum der statistischen Untersuchung steht. Alternative Begriffe sind, je nach Kontext: Untersuchungseinheit, Proband, Merkmalsträger. Siehe Zimmermann-Janschitz (2014), Kapitel 3.1.3 für Beispiele. Jene Eigenschaft eines Untersuchungselements, die für die statistische Untersuchung von Bedeutung ist, wird als Merkmal \\(X\\) des Elements bezeichnet. Eine statistische Einheit weist mindestens ein Merkmal auf, kann aber ebenso durch mehrere Merkmale gekennzeichnet sein. Alternative Begriffe sind: Variable, Indikator. Beispiele finden Sie wiederum in Zimmermann-Janschitz (2014), Kapitel 3.1.3. Die Merkmalsausprägungen \\(a_j\\) mit \\(j=1, \\ldots, m\\) eines Merkmals \\(X\\) umfassen jene Manifestationen, die ein Merkmal im Rahmen einer statistischen Untersuchung annehmen kann. Alternative Begriffe sind: Merkmalskategorien, Modalitäten. Z.B. kann das Merkmal Schneedeckenhöhe Werte von null (kein Schnee vorhanden) bis zu mehreren Metern einnehmen. Der Merkmalswert \\(x_i\\) mit \\(i=1, \\ldots, n\\) schließlich ist jener Wert, den ein Merkmal \\(X\\) in einer statistischen Untersuchung tatsächlich annimmt. Alternative Begriffe sind: Beobachtungswert, Datum (Plural: Daten). Beispiel: Tatsächliche Schneedeckenmessung an einem Punkt von 285,5cm. 2.2 Datenerhebung Leitfragen zu Zimmermann-Janschitz (2014), Kapitel 3.1.2: Was ist der Unterschied zwischen Primärdaten und Sekundärdaten? Was sind die Vor- und Nachteile? Was sind Metadaten? Wozu sind sie wichtig? Was ist der Unterschied zwischen Gesamterhebung und Teilerhebung? Zur Primärdatenerhebung hören Sie mehr von Henning Nuissl im Seminar Einführung in die Geographie. An dieser Stelle aber noch ein paar Worte zur Auswahl einer Stichprobe aus einer Grundgesamtheit. Aus statistischer Perspektive sind dabei prinzipiell drei Aspekte zu beachten: Repräsentativität: Eine Stichprobe sollte die Variabilität der Grundgesamtheit möglichst genau abbilden, z.B. bezüglich Demographien oder räumlicher Unterschiede. Zufälligkeit: Jede Merkmalsausprägung der Grundgesamtheit sollte die gleiche “Chance” haben ausgewählt zu werden. In der Praxis ist dies oft nur näherungsweise möglich. Stichprobenumfang: Eine Stichprobe sollte ausreichend groß sein. Mehr dazu in der schließenden Statistik (Kapitel 8 - 12). 2.3 Skalenniveaus Die Skalenniveaus von Daten bestimmen den Informationsgehalt der Daten und damit das Analyse- und Interpretationspotenzial. In der Reihenfolge Nominalskala - Ordinalskala - metrische Skalen werden jeweils zusätzliche mathematische Operationen erschlossen (s. Zimmermann-Janschitz 2014, Tabelle 3.8, S. 79). Qualitative, klassifikatorische Merkmale befinden sich auf der Nominalskala. Die Kategorien können verbale Bezeichnungen oder Zahlencodes sein (Achtung: Die Zahl ist in dem Fall ein Code und keine natürliche Zahl mit der gerechnet werden kann.) Die zulässige mathematische Operation ist der Vergleich, d.h. Merkmalswerte von statistischen Einheiten sind entweder gleich oder verschieden. Beispiel: Art des Vulkanausbruchs (Tabelle 2.1). Obwohl \\(1+2=3\\) ist, ist Lava plus Gestein nicht gleich Gas! Tabelle 2.1: Art des Vulkanausbruchs. Lava Gestein Gas Asche 1 2 3 4 Qualitative, komparative Merkmale (Rangmerkmale) finden Sie auf der Ordinalskala. Wieder können die Kategorien verbale Bezeichnungen sein oder mittels Zahlen codiert. Die Zulässigen mathematischen Operationen sind der Vergleich sowie Wertung/Reihung/Ordnung. Es sind keine Aussagen über Distanz oder Ähnlichkeit benachbarter Merkmalsausprägungen möglich. Beispiel: Komfort der Unterkunft (Tabelle 2.2). Der Komfort eines ***Hotels ist größer als der Komfort eines *Hotels, aber nicht 3x so groß! Tabelle 2.2: Komfort der Unterkunft. Jugendherberge *Hotel **Hotel ***Hotel 0 1 2 3 Quantitative Merkmale befinden sich auf metrischen Skalen. Sie werden mit reellen Zahlen bezeichnet. Die zulässigen mathematischen Operationen sind der Vergleich, Wertung/Reihung/Ordnung sowie Addition/Subtraktion und im Falle der Rationalskala auch Multiplikation/Division. Die Unterscheidung Intervallskala und Rationalskala (Verhältnisskala) ist für uns nicht so wichtig. Intervallskalen fehlt ein natürlicher Nullpunkt und daher ist die Berechnung von Relationen nicht möglich. Sie kann aber auf einen Referenznullpunkt umgerechnet werden, wodurch Multiplikation und Division möglich wedren. Wenn wir also in diesem Kurs von einer metrischen Skala sprechen dann sind die mathematischen Operationen Vergleich, Wertung/Reihung/Ordnung, Addition/Subtraktion und Multiplikation/Division alle zulässig. Literatur "],
["03-haeufigkeit.html", "Kapitel 3 Häufigkeiten und Lageparameter 3.1 Ziel der deskriptiven Statistik 3.2 Häufigkeiten 3.3 Lageparameter", " Kapitel 3 Häufigkeiten und Lageparameter Mit diesem Kapitel des Skriptes steigen wir in die deskriptive Statistik ein. Lesen Sie hierzu bitte Kapitel 3.2.1 und 3.2.2 von Zimmermann-Janschitz (2014). Im Folgenden finden Sie wie gehabt Leitfragen und Ergänzungen zu diesen Kapiteln. 3.1 Ziel der deskriptiven Statistik Jene Eigenschaft eines Untersuchungselements, die für die statistische Untersuchung von Bedeutung ist, wird als Merkmal \\(X\\) des Elements bezeichnet. Der Merkmalswert \\(x_i\\) mit \\(i=1,\\ldots,n\\) ist jener Wert, den ein Merkmal \\(X\\) in einer statistischen Untersuchung tatsächlich annimmt. Ziel der deskriptiven Statistik ist es, die Verteilung der Merkmalswerte eines Merkmals über den möglichen Wertebereich (Ausprägungen) mit einzelnen Parametern näher zu charakterisieren. Z.B. die Anzahl der Ausbrüche eines Vulkans an verschiedenen Tagen. Die Parameter sind: Lageparameter: Maße der zentralen Tendenz einer Verteilung (s. dieses Kapitel) Streuungsparameter: Maße der Variabilität einer Verteilung (s. Kapitel 4) Schiefe und Wölbung einer Verteilung (s. Kapitel 4) Dazu brauchen wir erstmal Begriffe wie absolute Häufigkeit, relative Häufigkeit und Summenhäufigkeit, sowie Diagramme wie Histogramme und Boxplots, die Verteilungen graphisch darstellen. 3.2 Häufigkeiten Schauen wir uns dazu die Reisedaten an, die Sie als Studierende in diesem Jahr eingegeben haben, und zwar nur die Entfernungen (der R-Code wird in den PC-Übungen schnell klar werden): # Paket laden, das für das Einlesen von xlsx gebraucht wird library(&quot;readxl&quot;) # Daten einlesen reisedat &lt;- read_excel(&quot;data/Daten_Distanz_Stationen.xlsx&quot;) # in Zahlen und data.frame umwandeln reisedat &lt;- as.data.frame(apply(reisedat, 2, as.numeric)) # Ausgabe von reisedat$distanz reisedat$distanz ## [1] 25.01 19.40 16.70 9.20 15.80 13.30 11.00 19.42 ## [9] 14.50 22.70 20.00 5.60 14.00 16.90 24.40 10.00 ## [17] 14.00 6.80 2.50 15.60 8.90 21.00 8.49 8.60 ## [25] 17.70 29.00 29.00 21.00 11.30 10.30 3.60 15.00 ## [33] 40.00 40.00 50.00 12.60 16.00 8.20 7.30 9.58 ## [41] 17.20 18.80 14.70 10.00 11.00 2.90 14.40 8.60 ## [49] 9.85 13.40 20.00 6.00 13.40 19.00 5.20 5.50 ## [57] 15.80 9.12 18.00 8.20 14.40 5.10 26.00 9.70 ## [65] 30.80 6.90 8.20 18.00 7.60 9.80 12.50 28.20 ## [73] 8.80 17.30 Das ist eine ungeordnete Reihe von 74 Datenpunkten, den sogenannten Rohdaten. Wenn wir die Rohdaten jetzt ordnen und in Klassen einteilen können wir absolute Häufigkeiten bestimmen, gewissermaßen durch Abzählen: # reisedat$distanz aufsteigend ordnen und ausgeben sort(reisedat$distanz) ## [1] 2.50 2.90 3.60 5.10 5.20 5.50 5.60 6.00 ## [9] 6.80 6.90 7.30 7.60 8.20 8.20 8.20 8.49 ## [17] 8.60 8.60 8.80 8.90 9.12 9.20 9.58 9.70 ## [25] 9.80 9.85 10.00 10.00 10.30 11.00 11.00 11.30 ## [33] 12.50 12.60 13.30 13.40 13.40 14.00 14.00 14.40 ## [41] 14.40 14.50 14.70 15.00 15.60 15.80 15.80 16.00 ## [49] 16.70 16.90 17.20 17.30 17.70 18.00 18.00 18.80 ## [57] 19.00 19.40 19.42 20.00 20.00 21.00 21.00 22.70 ## [65] 24.40 25.01 26.00 28.20 29.00 29.00 30.80 40.00 ## [73] 40.00 50.00 # absolute Häufigkeiten bestimmen für Klassen von 0 bis 55km, mit Breite 5km hist(reisedat$distanz, breaks = seq(0, 55, 5), main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) Diese Darstellung ist ein Histogramm. Dazu später mehr. Das Ordnen geschieht bei der Errechnung des Histogramms automatisch, hier haben wir den dazugehörigen R-Code nur der Anschaulichkeit halber eingefügt. Die absolute Häufigkeit, bezeichnet mit \\(h_j\\) für \\(h\\left(a_j\\right)\\) und \\(j=1,\\ldots,m\\), gibt also die Anzahl der statistischen Einheiten in einer Stichprobe an, welche die Merkmalsausprägung \\(a_j\\) für ein Merkmal \\(X\\) annehmen. In unserem Beispiel haben wir die Merkmalsausprägungen durch Klassifizierung gewissermaßen “künstlich” erzeugt, da die Menge der Ausprägungen des Merkmals “Entfernung” ja nicht abzählbar ist. Im Beispiel der Anzahl Vulkanausbrüche in Zimmermann-Janschitz (2014) gibt es dagegen abzählbare Merkmalsausprägungen. Die Summe der absoluten Häufigkeiten ist der Stichprobenumfang \\(n\\): \\[\\sum_{j=1}^{m}h_j=n \\quad\\text{mit}\\quad m\\leq n\\] Die relative Häufigkeit, bezeichnet mit \\(f_j\\) für \\(f\\left(a_j\\right)\\) und \\(j=1,\\ldots,m\\), gibt dann den Anteil der statistischen Einheiten an einer Stichprobe an, welche die Merkmalsausprägung \\(a_j\\) für ein Merkmal \\(X\\) annehmen: \\[f_j=f\\left(a_j\\right)=\\frac{h\\left(a_j\\right)}{n}\\] Das Histogramm bleibt gleich, nur dass die vertikale Achse anders skaliert ist. Da das in der hist()-Funktion nicht vorgesehen ist, ist der R-Code etwas länger: # Histogramm berechnen ohne Output h &lt;- hist(reisedat$distanz, breaks = seq(0, 55, 5), plot = FALSE) # absolute in relative Häufigkeiten umrechnen h$counts &lt;- h$counts / sum(h$counts) # Histogrammdaten plotten plot(h, freq = TRUE, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;relative Häufigkeit&quot;) Interpretation: Der ersten Balken z.B. geht bis knapp unter 0.05, d.h. knapp 5% der Entfernungsdaten haben Werte zwischen 0 und 5km. Wenn wir uns das Histogramm der absoluten Häufigkeiten weiter oben anschauen, dann sehen wir, dass das 3 von 74 Datenpunkten sind. Die Summe der relativen Häufigkeiten ist 1, was 100% entspricht: \\[\\sum_{j=1}^{m}f_j=1 \\quad\\text{mit}\\quad m\\leq n\\] Kommen wir nun zu den Summenhäufigkeiten, auch genannt kumulative Häufigkeit oder kumulierte Häufigkeit. Die absolute Summenhäufigkeit einer Merkmalsausprägung \\(a_j\\) ist die Anzahl der Merkmalswerte, die kleiner oder gleich \\(a_j\\) sind. Die relative Summenhäufigkeit von \\(a_j\\) ist dementsprechend der Anteil der Merkmalswerte, die kleiner oder gleich \\(a_j\\) sind. Am besten visualisieren wir das kurz in R: # Histogramm berechnen ohne Output h &lt;- hist(reisedat$distanz, breaks = seq(0, 55, 5), plot = FALSE) # Häufigkeiten kumuliert aufsummieren h$counts &lt;- cumsum(h$counts) # Histogrammdaten plotten plot(h, freq = TRUE, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Summenhäufigkeit&quot;) # absolute Summenhäufigkeiten in relative umrechnen h$counts &lt;- h$counts / max(h$counts) # Histogrammdaten plotten plot(h, freq = TRUE, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;relative Summenhäufigkeit&quot;) Die Häufigkeiten der Klassen sind hier kumuliert aufsummiert, d.h. der letzte Balken ganz rechts hat die absolute Höhe 74, die Gesamtzahl der Datenpunkte \\(n\\), bzw. die relative Höhe 1 (100%). Sehen die dazu auch das Beispiel in Zimmermann-Janschitz (2014), Tabelle 3.10 auf Seite 87. Abschließend noch ein paar Worte zur Klassifizierung. Äquidistante Klassen, d.h. Klassen gleicher Breite, sind grundsätzlich zu bevorzugen. In R können Sie eine gewünschte Anzahl Klassen angeben, die dann äquidistant über den Wertebereich verteilt werden. Das ist sinnvoll für einen ersten Eindruck. Die Grundeinstellung von 10 Klassen ist dabei meist ausreichend. Im Laufe der Analyse wird es manchmal sinnvoller sein, bestimmte Klassen vorzugeben, auch (oder gerade) wenn manche Klasse in der betrachteten Stichprobe nicht vorkommen. Auch das ist in R möglich, indem Sie die Klassengrenzen (“breaks”) angeben. Probieren wir das in R aus: # Klassenbreite 5km hist(reisedat$distanz, breaks = seq(0, 60, 5), main = &quot;5km Klassenbreite&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) # Klassenbreite 10km hist(reisedat$distanz, breaks = seq(0, 60, 10), main = &quot;10km Klassenbreite&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) # Klassenbreite 20km hist(reisedat$distanz, breaks = seq(0, 60, 20), main = &quot;20km Klassenbreite&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) Je größer die Klassenbreite, desto mehr Nuancen der Verteilung der Werte gehen verloren. Je kleiner die Klassenbreite, desto mehr Lücken entstehen im Histogramm und die generelle Form der Verteilung tritt in den Hintergrund. Eine geeignete mittlere Klassenbreite wird man nur durch Ausprobieren hinbekommen. Siehe aber Zimmermann-Janschitz (2014), S. 91 - 102 für Richtlinien zur Klassenbildung. 3.3 Lageparameter Lageparameter sind Maße der zentralen Tendenz einer Häufigkeitsverteilung. Siehe Zimmermann-Janschitz (2014), Kapitel 3.2.2. Wichtig sind uns in dieser Veranstaltung der Modus, das arithmetische Mittel und der Median, weniger das harmonische Mittel und das geometrische Mittel, die Sie aber bei Zimmermann-Janschitz (2014) nachlesen können. Der Modus \\(\\bar x_{mod}\\) entspricht jener Merkmalsausprägung \\(a_j\\), die in der Häufigkeitsverteilung (lokal) am häufigsten vorkommt. In unseren Entfernungsdaten wäre das der Wert \\(7.5\\): h &lt;- hist(reisedat$distanz, breaks = seq(0, 55, 5), plot = FALSE) plot(h, freq = TRUE, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) # gib Klassenmitte aus (an der Stelle, wo die Häufigkeiten gleich dem Maximum sind) h$mids[h$counts == max(h$counts)] ## [1] 7.5 Bei “künstlichen” Klassen wie in unserem Beispiel wird typischerweise die Mitte der häufigsten Klasse angegeben. Sie sehen also, der Modus ist in diesem Fall abhängig von der gewählten Klassifizierung und nicht eindeutig. In der obigen Darstellung sehen wir außerdem weitere lokale Maxima bei \\(17.5\\), \\(27.5\\), \\(37.5\\) und \\(47.5\\), es handelt sich also in dieser Klassifizierung um eine multimodale Verteilung - eine Verteilung mit mehreren Modi. Ein Sonderfall der multimodalen Verteilung stellt die sogenannte bimodale Verteilung dar, bei der zwei lokale Maxima vorliegen. Das arithmetische Mittel \\(\\bar x\\) der Merkmalswerte \\(x_1, x_2, \\ldots, x_n\\) ist die Summe aller Merkmalswerte \\(x_i\\) relativ zur Stichprobengröße \\(n\\): \\[\\bar x=\\frac{\\sum_{i=1}^{n}x_i}{n}\\] Liegen absolute oder relative Häufigkeiten für die Merkmalsausprägungen \\(a_j\\) vor, kann das arithmetische Mittel \\(\\bar x\\) gewichtet berechnet werden: \\[\\bar x=\\frac{\\sum_{j=1}^{m}a_j\\cdot h_j}{n}=\\sum_{j=1}^{m}a_j\\cdot f_j\\quad\\text{mit}\\quad m\\leq n\\] Berechnen wir das arithmetische Mittel für unsere Entfernungsdaten mit der Funktion mean(): # berechne arithmetisches Mittel für Variable &quot;Distanz&quot; in km mean(reisedat$distanz) ## [1] 14.98 Andere Mittelwerte sind das geometrische Mittel und das harmonische Mittel, siehe Zimmermann-Janschitz (2014), S. 126 - 134. Der Median \\(\\bar x_{med}\\) schließlich entspricht jenem Merkmalswert \\(x_j\\) in einer Häufigkeitsverteilung, der eine geordnete Reihe von Merkmalswerten \\(x_1, x_2, \\ldots, x_n\\) in zwei gleich große Wertebereiche teilt. Für eine ungerade Anzahl von Merkmalswerten entspricht der Median dem mittleren Wert: \\[\\bar x_{med}=x_{\\frac{n+1}{2}}\\] Für eine gerade Anzahl von Merkmalswerten wird der Median aus dem arithmetischen Mittel der beiden mittleren Werte errechnet: \\[\\bar x_{med}=\\frac{x_{\\frac{n}{2}}+x_{\\frac{n}{2}+1}}{2}\\] Berechnen wir den Median für unsere Entfernungsdaten mit der Funktion median(): median(reisedat$distanz) ## [1] 13.7 Der Median wird auch 0.5-Quantil genannt. Allgemein entspricht ein p-Quantil \\(\\bar x_p\\) mit \\(0\\leq p\\leq 1\\) jenem Merkmalswert \\(x_j\\) in einer Häufigkeitsverteilung, der eine geordnete Reihe von Merkmalswerten \\(x_1, x_2, \\ldots, x_n\\) in zwei Wertebereiche teilt, so dass ein Anteil \\(p\\) der Werte kleiner oder gleich \\(x_j\\) ist. Ist das Produkt \\(n\\cdot p\\) nicht ganzzahlig, wird für \\(j\\) die dem Produkt nächstgrößere Zahl verwendet: \\[\\bar x_p=x_j\\] Ist das Produkt \\(n\\cdot p\\) ganzzahlig, dann ist \\(j=n\\cdot p\\): \\[\\bar x_p=\\frac{x_j+x_{j+1}}{2}\\] Auf Quantile werden wir im Zuge theoretischer Verteilungen noch näher zu sprechen kommen (s. Kapitel 7). Literatur "],
["04-streuung.html", "Kapitel 4 Streuungsparameter, Schiefe und Wölbung 4.1 Streuungsparameter 4.2 Schiefe und Wölbung von Häufigkeitsverteilungen", " Kapitel 4 Streuungsparameter, Schiefe und Wölbung 4.1 Streuungsparameter Lesen Sie dazu bitte Kapitel 3.2.3 von Zimmermann-Janschitz (2014). Streuungsparameter sind Maße der Variabilität einer Häufigkeitsverteilung. Uns interessieren hier v.a. Spannweite, Quartilsabstand, Varianz und Standardabweichung und Variationskoeffizient, weniger durchschnittliche absolute Abweichung, da wir letztere kaum in der Praxis sehen. Spannweite und Quartilsabstand lassen sich am besten mit einem sogenannten Box-Whisker-Plot, kurz einfach Boxplot, verdeutlichen (Abbildung 4.1). Ein Boxplot fasst die Verteilung der Werte eines Merkmals (in einer Stichprobe) zusammen. Die Spannweite ist der Abstand zwischen Minimum und Maximum der Merkmalswerte. Der Quartilsabstand ist der Abstand zwischen 0.25-Quantil und 0.75-Quantil; in diesem Bereich liegen 50% der Merkmalswerte (0.75 - 0.25). 0.25-Quantil, 0.5-Quantil (Median) und 0.75-Quantil heißen auch 1., 2. und 3. Quartil, weil sie den Wertebereich in vier gleichgroße Teile teilen: zwischen Minimum und 0.25-Quantil liegen 25% der Merkmalswerte, zwischen 0.25-Quantil und Median 25%, zwischen Median und 0.75-Quantil 25% und zwischen 0.75-Quantil und Maximum ebenfalls 25% aller Merkmalswerte. Ebenso gibt es auch Quintile usw., diese sind aber in der Praxis kaum von Bedeutung. Ein Boxplot kann horizontal (wie hier) und vertikal dargestellt werden. Abbildung 4.1: Boxplot mit Quartilsabstand und Spannweite. Der Boxplot ist eine vereinfachte Darstellung eines Histogramms. Schauen Sie sich dazu bitte Kapitel 4.3.6 von Zimmermann-Janschitz (2014) an, v.a. Abbildung 4.10. Erkennen Sie welcher Boxplot in 4.10b zu welchem Histogramm in 4.10c gehört? Die Entsprechung können Sie auch in unseren Reisedaten sehen (hier sowohl “Distanz” als auch “Stationen”): # Histogramm &quot;Distanz&quot; in km hist(reisedat$distanz, breaks = seq(0, 55, 5), main = &quot;&quot;, xlab = &quot;Entfernung (km)&quot;, ylab = &quot;absolute Häufigkeit&quot;) # Histogramm &quot;Stationen&quot; hist(reisedat$stationen, breaks = seq(0, 55, 5), main = &quot;&quot;, xlab = &quot;Anzahl Stationen&quot;, ylab = &quot;absolute Häufigkeit&quot;) # Boxplot &quot;Distanz&quot; in km boxplot(reisedat$distanz, range = 0, horizontal = TRUE, ylim = c(0, 55), xlab = &quot;Entfernung (km)&quot;) # Boxplot &quot;Stationen&quot; boxplot(reisedat$stationen, range = 0, horizontal = TRUE, ylim = c(0, 55), xlab = &quot;Anzahl Stationen&quot;) Sowohl “Distanz” als auch “Stationen” sind schief verteilt, die zentralen 50% der Verteilung - die “Box” im Boxplot - befinden sich links der Mitte. Für das Verständnis von Verteilungen in Kapitel 7 ist es wichtig, dass sie den Zusammenhang zwischen Histogramm und Boxplot verstehen! Nun zu den weiteren Streuungsparametern. Die Varianz \\(s^2\\) ist die mittlere (“durchschnittliche”) quadrierte Abweichung der Merkmalswerte \\(x_i\\quad\\left(i=1, 2, \\ldots, x_n\\right)\\) vom arithmetischen Mittel \\(\\bar x\\): \\[s^2=\\frac{\\sum_{i-1}^{n}\\left(x_i-\\bar x\\right)^2}{n-1}\\] Genau genommen ist das die korrigierte Varianz, wo durch \\(n-1\\) geteilt wird und nicht durch \\(n\\) wie man bei einer Mittelung erwarten könnte. Das Teilen durch \\(n-1\\) garantiert eine optimale Schätzung der Varianz der Grundgesamtheit anhand der Stichprobe - mehr dazu in der schließenden Statistik. Der Nenner \\(n-1\\) wird Anzahl Freiheitsgrade genannt und bezeichnet die Anzahl der Werte in einer Stichprobe, die für die Berechnung des Parameters (hier Varianz) frei zur Verfügung stehen. Im Fall der Varianz ist ein Wert der Stichprobe bereits “belegt” – durch das arithmetische Mittel. Daher reduziert sich die Zahl der Elemente der Stichprobe, die in die Berechnung eingehen um eins. Die Standardabweichung \\(s\\) ist die Quadratwurzel der mittleren quadrierten Abweichung der Merkmalswerte \\(x_i\\quad\\left(i=1, 2, \\ldots, x_n\\right)\\) vom arithmetischen Mittel \\(\\bar x\\), d.h. die Quadratwurzel der Varianz: \\[s=\\sqrt{s^2}=\\sqrt{\\frac{\\sum_{i-1}^{n}\\left(x_i-\\bar x\\right)^2}{n-1}}\\] Die Standardabweichung besitzt die gleiche Einheit wie die Merkmalswerte und ist deshalb einfacher zu interpretieren als die Varianz. Sie drückt die Streuung der Merkmalswerte um den Mittelwert bzw. deren Abweichung vom Mittelwert in einer anschaulichen Größe aus. Je größer die Werte der Standardabweichung sind, desto mehr streuen die Daten. Der Variationskoeffizient \\(v\\) einer Häufigkeitsverteilung mit den Merkmalswerten \\(x_i\\quad\\left(i=1, 2, \\ldots, x_n\\right)\\) schließlich ist die Standardabweichung \\(s\\) im Verhältnis zum Mittelwert \\(\\bar x\\): \\[v=\\frac{s}{\\bar x}\\] Der Variationskoeffizient setzt die Streuung der Merkmalswerte in unmittelbare Relation zum arithmetischen Mittel. Dadurch werden unterschiedliche Verteilungen vergleichbar. Schauen wir uns die Streungsparameter für die Reisedaten mittels R an: # &quot;Distanz&quot; in km # 1) arithmetisches Mittel dbar &lt;- mean(reisedat$distanz) # 2) Varianz s2d &lt;- var(reisedat$distanz) # 3) Standardabweichung sd &lt;- sqrt(s2d) # oder sd &lt;- sd(reisedat$distanz) # 4) Variationskoeffizient vd &lt;- sd / dbar # Ergebnisse: print(c(dbar, s2d, sd, vd)) ## [1] 14.9834 77.9118 8.8268 0.5891 # &quot;Stationen&quot; # 1) arithmetisches Mittel sbar &lt;- mean(reisedat$stationen) # 2) Varianz s2s &lt;- var(reisedat$stationen) # 3) Standardabweichung ss &lt;- sd(reisedat$stationen) # 4) Variationskoeffizient vs &lt;- ss / sbar # Ergebnisse: print(c(sbar, s2s, ss, vs)) ## [1] 13.8243 96.9139 9.8445 0.7121 Die Variable “Stationen” hat im Vergleich zu “Distanz” eine etwas größere Varianz bei etwas geringerem Mittelwert. Daher ist der Variationskoeffizient größer. 4.2 Schiefe und Wölbung von Häufigkeitsverteilungen Lesen Sie dazu bitte Kapitel 3.2.5 von Zimmermann-Janschitz (2014). Die Schiefe \\(a_3\\) einer Häufigkeitsverteilung von Merkmalswerten \\(x_1, x_2, \\ldots, x_n\\) mit dem arithmetischen Mittel \\(\\bar x\\) und der Standardabweichung \\(s\\) bezeichnet die Abweichung der Verteilung der Merkmalswerte von der symmetrischen Form: \\[a_3=\\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^3}{n\\cdot s^3}\\] Für eine symmetrische Verteilung gilt: \\[a_3=0\\quad \\bar x_{mod}=\\bar x_{med}=\\bar x\\] D.h. Modus, Median und Arithmetisches Mittel sind identisch. Für eine sogenannte rechtsschiefe (linkssteile) Verteilung gilt: \\[a_3&gt;0\\quad \\bar x_{mod}&lt;\\bar x_{med}&lt;\\bar x\\] Für eine linkschiefe (rechtssteile) Verteilung gilt: \\[a_3&lt;0\\quad \\bar x_{mod}&gt;\\bar x_{med}&gt;\\bar x\\] Wie wir an Histogramm und Boxplot der Entfernungsdaten bereits gesehen haben, sind die Verteilungen der Merkmale “Distanz” und “Stationen” rechtsschief: # für diese Berechnung brauchen wir das Paket &quot;moments&quot; library(moments) # &quot;Distanz&quot; in km # 1) Schiefe skew_d &lt;- skewness(reisedat$distanz) # 2) Median med_d &lt;- median(reisedat$distanz) # 3) arithmetisches Mittel mean_d &lt;- mean(reisedat$distanz) # Ergebnisse: print(c(skew_d, med_d, mean_d)) ## [1] 1.516 13.700 14.983 Daraus folgt: Modus(=7.5) &lt; Median &lt; arithm. Mittelwert Das Merkmal &quot;Distanz&quot; ist rechtsschief verteilt. # &quot;Stationen&quot; # 1) Schiefe skew_s &lt;- skewness(reisedat$stationen) # 2) Median med_s &lt;- median(reisedat$stationen) # 3) arithmetisches Mittel mean_s &lt;- mean(reisedat$stationen) # Ergebnisse: print(c(skew_s, med_s, mean_s)) ## [1] 1.423 12.000 13.824 Daraus folgt: Modus(=7.5) &lt; Median &lt; arithm. Mittelwert Das Merkmal &quot;Stationen&quot; ist rechtsschief verteilt. Die Wölbung \\(a_4\\) einer Häufigkeitsverteilung von Merkmalswerten \\(x_1, x_2, \\ldots, x_n\\) mit dem arithmetischen Mittel \\(\\bar x\\) und der Standardabweichung \\(s\\) bestimmt die Steilheit einer Verteilung: \\[a_4=\\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^4}{n\\cdot s^4}-3\\] Die Subtraktion von \\(-3\\) dient der Standardisierung auf die sogenannte Normalverteilung, eine symmetrische, glockenförmige Verteilung (s. Zimmermann-Janschitz 2014, Kapitel 3.2.5). Mehr zur Normalverteilung in Kapitel 7. Für eine Normalverteilung gilt: \\[a_4=0\\] Für eine “spitzere” Verteilung als die Normalverteilung gilt: \\[a_4&gt;0\\] Für eine “flachere” Verteilung als die Normalverteilung gilt: \\[a_4&lt;0\\] Die Verteilungen der Merkmale unserer Reisedaten sind beide spitzer als die Normalverteilung, wobei beide Variablen wegen ihrer Rechtsschiefe ohnehin nicht mit der Normalverteilung vergleichbar sind: # Wölbung der &quot;Distanz&quot; kurtosis(reisedat$distanz) - 3 ## [1] 3.038 # Wölbung der &quot;Stationen&quot; kurtosis(reisedat$stationen) - 3 ## [1] 2.266 Literatur "],
["05-korrelation.html", "Kapitel 5 Korrelationsanalyse 5.1 Nominalskalierte Merkmale: Kontingenztabelle und Chi-Quadrat Statistik 5.2 Ordinalskalierte Merkmale: Rangkorrelationskoeffizient nach Spearman 5.3 Metrische Merkmale: Scatterplot (Streudiagramm) und Korrelationskoeffizient nach Bravais-Pearson", " Kapitel 5 Korrelationsanalyse Lesen Sie hierzu bitte Kapitel 3.4.2 von Zimmermann-Janschitz (2014). Die Fragen, die uns besonders interessieren sind: Gibt es einen Zusammenhang zwischen zwei Merkmalen? Wenn ja, wie kann dieser Zusammenhang charakterisiert werden? Wie stark ist der Zusammenhang zwischen den Merkmalen? Wie kann man den Zusammenhang visualisieren? Die Verfahren sind unterschiedlich für nominalskalierte, ordinalskalierte und metrische Merkmale. Wir werden diese jetzt nacheinander einführen. Die Themen können Sie in Mittag (2016), Kapitel 8 und 9 vertiefen. 5.1 Nominalskalierte Merkmale: Kontingenztabelle und Chi-Quadrat Statistik Wir wollen diese Methode am Beispiel des Volksentscheids Tegel von 2017 verdeutlichen.1 Der Beschlussentwurf lautete: “Der Flughafen Berlin-Tegel „Otto-Lilienthal“ ergänzt und entlastet den geplanten Flughafen Berlin Brandenburg „Willy Brandt“ (BER). Der Berliner Senat wird aufgefordert, sofort die Schließungsabsichten aufzugeben und alle Maßnahmen einzuleiten, die erforderlich sind, um den unbefristeten Fortbetrieb des Flughafens Tegel als Verkehrsflughafen zu sichern!” Wir stellen uns die Frage: Gibt es einen Zusammenhang zwischen Abstimmungsverhalten und Bezirk? Man könnte meinen, dass Anwohner*innen in der Einflugschneise von Tegel eher für “nein” stimmten (d.h. für die Schließung), Anwohner*innen in der Einflugschneise des neuen BER dagegen eher für “ja” (d.h. gegen die Schließung). Überlegungen wie diese motivieren unsere Fragestellung. Die Kontingenztabelle (oder Kreuztabelle) zu diesem Beispiel finden Sie in Abbildung 5.1. Abbildung 5.1: Kontingenztabelle zum Abstimmungsverhalten im Volksentscheid Tegel 2017 nach Bezirk. Quelle: https://www.wahlen-berlin.de/wahlen/BU2017/afspraes/ve/index.html. In dieser Tabelle stehen absolute Häufigkeiten (vgl. Kapitel 3) für alle Kombinationen von Merkmalsausprägungen der Variablen \\(X\\) und \\(Y\\). \\(X\\) bezeichnet “Bezirk” mit den Ausprägungen \\(a_1, a_2, \\ldots, a_n\\). \\(Y\\) bezeichnet “Abstimmungsverhalten” mit den Ausprägungen \\(b_1\\) (“ja”) und \\(b_2\\) (“nein”). Die Spalten- bzw. Zeilensummen sind die sogenannten Randverteilungen von \\(Y\\) und \\(X\\). Ganz unten rechts steht der Stichprobenumfang (die Anzahl der Personen, die abgestimmt haben). Der Stichprobenumfang kann sowohl als Summe der Randverteilungswerte von \\(X\\), als auch als Summe der Randverteilungswerte von \\(Y\\) errechnet werden. Die allgemeine Notation der absoluten Häufigkeiten einer Kontingenztabelle finden Sie in Abbildung 5.2. In dieser Notation stehen \\(h_{\\cdot j}\\) und \\(h_{i \\cdot}\\) für die Werte der beiden Randverteilungen. Abbildung 5.2: Notation der absoluten Häufigkeiten in einer Kontingenztabelle. Nach: Mittag (2016). Wir können die Kontingenztabelle für dieses Beispiel auch mit relativen Häufigkeiten darstellen (Abbildung 5.3). Abbildung 5.3: Kontingenztabelle zum Abstimmungsverhalten im Volksentscheid Tegel mit relativen Häufigkeiten. Als nächstes müssen wir den Begriff der bedingten relativen Häufigkeit einführen. Die bedingte relative Häufigkeit ist die Häufigkeit einer Ausprägung des einen Merkmals relativ zur Häufigkeit einer bestimmten Ausprägung des zweiten Merkmals, d.h. “bedingt” dadurch, dass wir uns auf eine Ausprägung des zweiten Merkmals festlegen. Relative Häufigkeiten existieren somit in zwei “Richtungen” in einer Kontingenztabelle (Abbildung 5.4). Abbildung 5.4: Illustration relativer Häufigkeiten in einer Kontingenztabelle. Links: Absolute Häufigkeiten der Ausprägungen von \\(X\\) unter der Bedingung \\(Y=b_j\\). Rechts: Absolute Häufigkeiten der Ausprägungen von \\(Y\\) unter der Bedingung \\(X=a_i\\). Nach: Mittag (2016). Die Formeln dazu sind: \\[f_Y\\left(b_j|a_i\\right)=\\frac{h_{ij}}{h_{i\\cdot}}\\quad \\text{mit}\\quad j=1, 2, \\ldots, m\\] \\(f_Y\\left(b_j|a_i\\right)\\) steht für die relative Häufigkeit einer Merkmalsausprägung \\(b_j\\) von \\(Y\\), bedingt durch eine bestimme Ausprägung \\(a_i\\) von \\(X\\). Die Bedingtheit wird mit dem Zeichen \\(|\\) ausgedrückt. Und weiterhin: \\[f_X\\left(a_i|b_j\\right)=\\frac{h_{ij}}{h_{\\cdot j}}\\quad \\text{mit}\\quad i=1, 2, \\ldots, k\\] \\(f_X\\left(a_i|b_j\\right)\\) steht für die relative Häufigkeit einer Merkmalsausprägung \\(a_i\\) von \\(X\\), bedingt durch eine bestimme Ausprägung \\(b_j\\) von \\(Y\\). Beispiel: Die relative Häufigkeit von “ja” unter der Bedingung “Charlottenburg-Wilmersdorf” ist (vgl. 5.1): \\[f_Y\\left(b_1|a_4\\right)=\\frac{h_{41}}{h_{4\\cdot}}=\\frac{109799}{158471}=0.69\\] D.h. 69% der Stimmen in Charlottenburg-Wilmersdorf waren “ja” Stimmen. Die relative Häufigkeit von “Charlottenburg-Wilmersdorf” unter der Bedingung “ja” dagegen ist: \\[f_X\\left(a_4|b_1\\right)=\\frac{h_{41}}{h_{\\cdot 1}}=\\frac{109799}{994916}=0.11\\] D.h. 11% der “ja” Stimmen kamen aus Charlottenburg-Wilmersdorf. Was hat das alles mit dem Zusammenhang zwischen Abstimmungsverhalten und Bezirk zu tun? Die bedingten relativen Häufigkeiten helfen uns, etwas über die Abhängigkeit bzw. Unabhängigkeit der beiden Merkmale zu sagen. Das läuft über das Konzept der empirischen Unabhängigkeit: Intuitiv werden wir Unabhängigkeit von \\(X\\) und \\(Y\\) als gegeben ansehen, wenn die Ausprägung eines Merkmals keinen Einfluss auf die Ausprägung des anderen Merkmals hat. Dies bedeutet, dass eine bedingte Häufigkeitsverteilung für ein Merkmal nicht davon abhängt, welche Merkmalsausprägung für das andere Merkmal als Bedingung vorausgesetzt wird. D.h. der Anteil von Charlottenburg-Wilmersdorf an den “ja” Stimmen sollte etwa so groß sein wie der Anteil von Charlottenburg-Wilmersdorf an den “nein” Stimmen. Bzw. der Anteil der “ja” Stimmen in Charlottenburg-Wilmersdorf sollte etwa so groß sein wie der Anteil der “ja” Stimmen in jedem anderen Bezirk. In Formeln ausgedrückt heißt das: \\[f_X\\left(a_i|b_1\\right)=f_X\\left(a_i|b_2\\right)=\\cdots=f_X\\left(a_i|b_m\\right)\\] Bzw.: \\[\\frac{h_{i1}}{h_{\\cdot 1}}=\\frac{h_{i2}}{h_{\\cdot 2}}=\\cdots=\\frac{h_{im}}{h_{\\cdot m}}=\\frac{h_{i\\cdot}}{n}\\] Allgemein formuliert: \\[\\frac{h_{ij}}{h_{\\cdot j}}=\\frac{h_{i\\cdot}}{n}\\] Das Umstellen dieser Gleichung nach \\(h_{ij}\\) ergibt: \\[h_{ij}=\\frac{h_{i\\cdot}\\cdot h_{\\cdot j}}{n}:=\\tilde h_{ij}\\] Diese absolute Häufigkeit, die wir mit \\(\\tilde h_{ij}\\) bezeichnen, ist die bei empirischer Unabhängigkeit erwartete absolute Häufigkeit. Das Zeichen \\(:=\\) bedeutet “wird definiert als”. Wir haben uns also mit Hilfe der relativen Häufigkeiten in der Kontingenztabelle und unserer Konzeption der empirischen Unabhängigkeit neue, bei Unabhängigkeit erwartete absolute Häufigkeiten an jeder Stelle der Kontingenztabelle generiert. Diese können wir jetzt mit den tatsächlichen absoluten Häufigkeiten in der Kontingenztabelle vergleichen (Abbildung 5.5). Sind die Abweichungen klein können wir von Unabhängigkeit der Merkmale ausgehen. Sind die Abweichungen groß können wir von Abhängigkeit ausgehen. Abbildung 5.5: Vergleich der beobachteten absoluten Häufigkeiten \\(h_{ij}\\) und der bei empirischer Unabhängigkeit erwarteten absoluten Häufigkeiten \\(\\tilde h_{ij}\\). Als Vergleichsmaß wird die sogenannte Chi-Quadrat Statistik \\(\\mathcal{X}^2\\) (oder quadratische Kontingenz) verwendet, und das ist dann auch unser Zusammenhangsmaß für nominalskalierte Merkmale. \\(\\mathcal{X}^2\\) ergibt sich aus der Differenz der tatsächlichen Häufigkeiten \\(h_{ij}\\) und den erwarteten Häufigkeiten bei empirischer Unabhängigkeit \\(\\tilde h_{ij}\\): \\[\\mathcal{X}^2=\\sum_{i=1}^{k}\\sum_{j=1}^{m}\\frac{\\left(h_{ij}-\\tilde h_{ij}\\right)^2}{\\tilde h_{ij}}\\] Bei Unabhängigkeit der Merkmale ist \\(\\mathcal{X}^2=0\\). Bei vollständiger Abhängigkeit ist \\(\\mathcal{X}^2=\\mathcal{X}_{max}^2\\), wobei \\(\\mathcal{X}_{max}^2=n\\cdot(M-1)\\) mit \\(M=\\min(k;m)\\). D.h. der maximal mögliche Wert der \\(\\mathcal{X}^2\\) Statistik ergibt sich durch die Dimensionen der Kontingenztabelle, die Zeilenanzahl \\(k\\) und die Spaltenanzahl \\(m\\). Dadurch können wir leider die Größenordnung der \\(\\mathcal{X}^2\\) Statistik schlecht intuitiv einordnen, was unser Beispiel verdeutlicht: Für den Volksentscheid Tegel ergibt sich ein Wert von \\(\\mathcal{X}^2=49895.1\\), wobei \\(\\mathcal{X}_{max}^2=1732940\\). Da \\(\\mathcal{X}^2\\) näher an \\(0\\) ist als an \\(1 732 940\\), scheint die Abhängigkeit der Merkmale “Bezirk” und “Abstimmungsverhalten” gering zu sein. Ob sie dennoch statistisch signifikant ist können wir erst mit einem sogenannten Chi-Quadrat-Test beurteilen, den wir als Teil der schließenden Statistik in Kapitel 11 kennenlernen. 5.2 Ordinalskalierte Merkmale: Rangkorrelationskoeffizient nach Spearman Hier ist das Beispiel des Zusammenhangs von Grünflächenanteil und Bildungsgrad in Stadbezirken in Zimmermann-Janschitz (2014), Kapitel 3.4.2, S. 274 - 278 sehr anschaulich. Das Merkmal “Grünflächenanteil” ist zwar metrisch skaliert, muss aber ordinal skaliert werden, um es mit dem Merkmal “Bildungsgrad” vergleichbar zu machen, das bereits ordinal vorliegt. S. Tabelle 3.36 (Zimmermann-Janschitz 2014, S. 277). Der Rangkorrelationskoeffizient nach Spearman \\(r_s\\) für \\(n\\) Wertepaare \\(\\left(x_i,y_i\\right)\\) mit \\(i=1, 2, \\ldots, n\\) wird dann berechnet durch: \\[r_s=1-\\frac{6\\cdot\\sum_{i=1}^{n}d_i^2}{n\\cdot\\left(n^2-1\\right)}\\] Wobei \\(d_i\\) die Differenz der Ränge der beiden Merkmale \\(X\\) und \\(Y\\) ist. Bei perfekter negativer Rangkorrelation ist \\(r_s=-1\\). Bei perfekter positiver Rangkorrelation ist \\(r_s=1\\). Wenn keine Rangkorrelation vorliegt ist \\(r_s=0\\). Bei dem Grünflächenanteil/Bildungsgrad Beispiel von Zimmermann-Janschitz (2014) ist \\(r_s=0.84\\), die beiden Merkmale sind also stark positiv korreliert. D.h. größere Anteile von Grünflächen sind tendenziell mit höheren Bildungsgraden assoziiert und kleinere Anteile von Grünflächen sind tendenziell mit geringeren Bildungsgraden assoziiert. 5.3 Metrische Merkmale: Scatterplot (Streudiagramm) und Korrelationskoeffizient nach Bravais-Pearson An dieser Stelle können wir zu unseren Reisedaten zurückkehren. Beide Merkmale (“Distanz” und “Stationen”) sind metrisch skaliert2 und somit können wir den Korrelationskoeffizienten nach Bravais-Pearson verwenden, der mehr Informationen über den Zusammenhang, nämlich Linearität, liefert. Einen ersten Eindruck verschafft der Scatterplot (auch Streudiagramm), den wir einfach mit der plot() Funktion in R generieren: plot(reisedat$stationen, reisedat$distanz, xlab = &quot;Anzahl Stationen&quot;, ylab = &quot;Entfernung (km)&quot;) An dieser Stelle fällt das Wertepaar \\((0, 40)\\) oben links auf. Hierbei handelt es sich wahrscheinlich um eine Person, die gar nicht in Berlin wohnt. Anstatt \\(0\\) wollen wir hier aber einen fehlenden Wert erzeugen, der in R mit “NA” markiert wird, da diese Frage auf die Person gar nicht zutraf. Darstellungen wie diese sind somit enorm hilfreich für die Überprüfung der Daten! Ersetzen wir also \\(0\\) Stationen mit “NA” und plotten erneut: # 0 mit NA ersetzen reisedat$stationen[reisedat$stationen == 0] &lt;- NA # plotten plot(reisedat$stationen, reisedat$distanz, xlab = &quot;Anzahl Stationen&quot;, ylab = &quot;Entfernung (km)&quot;) Aufgabe: Überlegen Sie kurz, wie stark die Korrelation hier sein wird! (Auflösung weiter unten.) Überlegen Sie außerdem, woher die Streuung in “Entfernung” kommen könnte. Die lineare Korrelation zweier metrischer Merkmale wird üblicherweise mit dem Produkt-Moment-Korrelationskoeffizient nach Bravais-Pearson \\(r_{x,y}\\) für \\(n\\) Wertepaare \\(\\left(x_i,y_i\\right)\\) mit \\(i=1, 2, \\ldots, n\\) berechnet. Er ergibt sich aus den Standardabweichungen \\(s_x\\) und \\(s_y\\) und der standardisierten Kovarianz \\(s_{x,y}\\) durch: \\[r_{x,y}=\\frac{s_{x,y}}{s_x\\cdot s_y}=\\frac{\\frac{1}{n-1}\\cdot\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)\\cdot\\left(y_i-\\bar y\\right)}{\\sqrt{\\frac{1}{n-1}\\cdot\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^2}\\cdot\\sqrt{\\frac{1}{n-1}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\bar y\\right)^2}}\\] In R mit der cor() Funktion: cor(reisedat$stationen, reisedat$distanz, use = &quot;complete.obs&quot;, method = &quot;pearson&quot;) ## [1] 0.8092 Das Argument use = \"complete.obs\" teilt R mit, nur vollständige Datenpaare zu verwenden, d.h. ohne “NA”, ansonsten wäre der Output “NA”. Anstatt method = \"pearson\" sind ebenfalls method = \"spearman\" und method = \"kendall\" möglich; beides sind Rangkorrelationskoeffizienten. In Abbildung 5.6 sehen Sie Werte des Korrelationskoeffizienten für verschiedene Zusammenhänge. Bei augenscheinlich nicht-linearen Zusammenhängen wählt man üblicherweise einen Rangkorrelationskoeffizienten, der dann aussagekräftiger ist. Abbildung 5.6: Werte des Korrelationskoeffizienten für verschiedene Zusammenhänge. Quelle: https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Correlation_examples.png/440px-Correlation_examples.png. Literatur "],
["06-wahrscheinlichkeit.html", "Kapitel 6 Grundlagen der Wahrscheinlichkeitsrechnung", " Kapitel 6 Grundlagen der Wahrscheinlichkeitsrechnung "],
["07-verteilungen.html", "Kapitel 7 Verteilungen", " Kapitel 7 Verteilungen "],
["08-schaetzen.html", "Kapitel 8 Schätzen von Verteilungsparametern", " Kapitel 8 Schätzen von Verteilungsparametern "],
["09-ttest.html", "Kapitel 9 t-Test", " Kapitel 9 t-Test "],
["10-ftest.html", "Kapitel 10 F-Test", " Kapitel 10 F-Test "],
["11-chi2test_kstest.html", "Kapitel 11 Chi-Quadrat- und Kolmogorow-Smirnow-Test", " Kapitel 11 Chi-Quadrat- und Kolmogorow-Smirnow-Test "],
["12-regression.html", "Kapitel 12 Lineare Regression 12.1 Definitionen 12.2 Beschreibung vs. Vorhersage 12.3 Ausblick: Weiterführende lineare Modelle 12.4 Lineare Regression 12.5 Signifikanz der Regression 12.6 Konfidenzintervalle und Signifikanz der Parameter 12.7 Güte der Modellanpassung", " Kapitel 12 Lineare Regression Für die lineare Regression kehren wir zu einer Frage aus Kapitel 5 zurück: Kann man die Entfernung zu Ihrem Wohnort mit der Anzahl Stationen, die Sie bis Adlershof brauchen statistisch vorhersagen? plot(reisedat$stationen, reisedat$distanz, xlab=&quot;Anzahl Stationen&quot;, ylab=&quot;Entfernung (km)&quot;) Wir erinnern uns, dass der Korrelationskoeffizient nach Bravais-Pearson aus Kapitel 5 0.81 war. Das Ziel ist nun, eine Gerade durch die Punktwolke zu legen, die den Trend beschreibt, so dass der Abstand der Punkte von der Geraden minimal ist. Es geht um 2 Variablen (Merkmale): die abhängige Variable \\(y\\) (im Bsp. Entfernung) die unabhängige Variable \\(x\\) (im Bsp. Anzahl Stationen) Die Variablen müssen metrisch skaliert sein.3 Wir wollen das generelle Verhalten von \\(y\\) mit \\(x\\) beschreiben. Eine Gerade stellt dabei das einfachste lineare Modell dar. 12.1 Definitionen Im Falle einer einzigen unabhängigen Variable lautet die Gleichung des linearen Models: \\[\\begin{equation} y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i \\quad \\text{mit} \\quad i=1,2,\\ldots,n \\tag{12.1} \\end{equation}\\] \\(y_i\\) bezeichnet den Wert der abhängigen Variable für Datenpunkt \\(i\\), und \\(x_i\\) den Wert der unabhängigen Variable für Datenpunkt \\(i\\). Der Parameter \\(\\beta_0\\) beschreibt den Achsenabschnitt der Geraden, also der Punkt an dem die Gerade die y-Achse schneidet. Der Parameter \\(\\beta_1\\) beschreibt die Steigung der Geraden. \\(\\epsilon_i\\) stellt das Residuum (also den Fehler) für Datenpunkt \\(i\\) dar (Abbildung 12.1). Abbildung 12.1: Lineare Regression: Definitionen. 12.2 Beschreibung vs. Vorhersage Der primäre Zweck einer Regressionsanalyse ist die Beschreibung (oder Erklärung) der Daten im Sinne einer allgemeinen Beziehung, die sich auf die Grundgesamtheit übertragen lässt, aus der diese Daten entnommen wurden. Da diese Beziehung eine Eigenschaft der Grundgesamtheit ist, sollte diese demnach auch Vorhersagen ermöglichen. Hierbei ist jedoch Vorsicht geboten. Betrachten Sie den Zusammenhang von Jahr und Weltrekordzeit für die in Abbildung 12.2 dargestellten Daten (Meile, Herren). Wenn, wie hier, die Zeit die unabhängige Variable ist, wird die Regression zu einer Form der Trendanalyse, die in diesem Fall eine Verringerung der Rekordzeit mit den Jahren anzeigt. (Die lm() Funktion und ihren Output werden wir weiter unten kennenlernen, hier geht es um die Grafiken.) # Daten laden mile &lt;- read.csv(&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mile/data/mile.csv&quot;, header=TRUE) # lineares Modell an Daten aus 1. Hälfte des 20. Jahrh. anpassen mile_fit1 &lt;- lm(seconds ~ year, data = mile[mile$year&lt;1950,]) # Informationen zu Parameterschätzern extrahieren coef(summary(mile_fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 912.2340 67.90140 13.435 3.615e-08 ## year -0.3439 0.03509 -9.798 9.059e-07 # lineares Modell an kompletten Datensatz anpassen mile_fit2 &lt;- lm(seconds ~ year, data = mile) coef(summary(mile_fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1006.876 21.532 46.76 1.361e-29 ## year -0.393 0.011 -35.73 3.780e-26 # Modellanpassung für 1. Hälfte des 20. Jahrh. plotten plot(mile$year[mile$year&lt;1950], mile$seconds[mile$year&lt;1950], xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Jahr&quot;, ylab = &quot;Weltrekord, Meile, Herren (Sekunden)&quot;) abline(coef(mile_fit1), lwd = 3, col = &quot;red&quot;) # Extrapolation für 2. Hälfte des 20. Jahrh. plotten plot(mile$year, mile$seconds, xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Jahr&quot;, ylab = &quot;Weltrekord, Meile, Herren (Sekunden)&quot;) abline(coef(mile_fit1), lwd = 3, col = &quot;red&quot;) # Modellanpassung für Gesamtdaten bis 2050 plotten plot(mile$year, mile$seconds, xlim = c(1900, 2050), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Jahr&quot;, ylab = &quot;Weltrekord, Meile, Herren (Sekunden)&quot;) abline(coef(mile_fit2), lwd = 3, col = &quot;red&quot;) Abbildung 12.2: Links: Trend des Weltrekords “Meile, Herren” in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends für die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: Wainer (2009) Wir sehen, dass sich der Weltrekord in der ersten Hälfte des 20. Jahrhunderts linear verbesserte (Abbildung 12.2, links). Dieser Trend passt auch für die zweite Hälfte des 20. Jahrhunderts bemerkenswert gut (Abbildung 12.2, Mitte). Wie lange kann sich der Weltrekord jedoch noch mit der gleichen Rate verbessern (Abbildung 12.2, rechts)? Dieses Beispiel zeigt deutlich die Anwendbarkeit von Regressionen für Vorhersagen innerhalb bestimmter Grenzen, zeigt jedoch gleichzeitig die Grenzen dieser einfachen Modelle für längere Vorhersagen (z.B. in Zeit und Raum). Im Falle des Weltrekords würden wir erwarten, dass die Verbesserungsrate mit der Zeit abnimmt, d.h. dass die Kurve abflacht, was ein nichtlineares Modell erfordert. 12.3 Ausblick: Weiterführende lineare Modelle Wenn wir über das lineare Modell sprechen, ist die abhängige Variable immer metrisch skaliert, während die unabhängigen Variablen metrisch, nominal/ordinal oder gemischt sein können. Im Prinzip kann jede dieser Varianten mathematisch gleich behandelt werden, d.h. alle können z.B. mit der lm() Funktion in R analysiert werden. Allerdings haben sich historisch gesehen unterschiedliche Bezeichnungen für diese Varianten etabliert, die hier erwähnt werden sollen, um Verwirrung zu vermeiden (Tabellen 12.1 und 12.2). Tabelle 12.1: Historische Namen für die Varianten des linearen Modells, je nachdem, ob die unabhängigen Variablen metrisch, nominal/ordinal oder gemischt sind. Die abhängige Variable ist immer metrisch skaliert. unabhängige Variable(n)metrisch unabhängige Variable(n)nominal/ordinal unabhängige Variable(n)gemischt Regression Varianzanalyse(ANOVA) Kovarianzanalyse(ANCOVA) Tabelle 12.2: Historische Namen für Regression, je nachdem, ob wir eine oder mehrere unabhängige Variablen und eine oder mehrere abhängige Variablen haben. 1 unabhängige Variable &gt;1 unabhängige Variable 1 abhängige Variable Regression Multiple Regression &gt;1 abhängige Variable Multivariate Regression Multivariate multiple Regression 12.4 Lineare Regression Wie soll nun die Gerade durch die Punktwolke gelegt werden, d.h. welche Werte sollen Achsenabschnitt \\(\\beta_0\\) und Steigung \\(\\beta_1\\) annehmen? Typischerweise werden Regressionsprobleme gelöst, indem die Summe der quadratischen Abweichungen zwischen der Regressionsgeraden und den Datenpunkten minimiert wird - die sogenannte Kleinste-Quadrate-Schätzung. Die Summe der quadratischen Abweichungen wird auch als \\(SSE\\) bezeichnet (Sum of Squared Errors). Grafisch gesehen probieren wir in Abbildung 12.1 verschiedene Geraden mit unterschiedlichen Achsenabschnitten \\(\\beta_0\\) und Steigungen \\(\\beta_1\\) aus und wählen diejenige, bei der die Summe aller vertikalen Abstände \\(\\epsilon_i\\) zum Quadrat am kleinsten ist. Mathematisch ist \\(SSE\\) definiert als: \\[\\begin{equation} SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2 \\tag{12.2} \\end{equation}\\] Das Residuum \\(\\epsilon_i\\) ist also gleich \\(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\), dem vertikalen Abstand zwischen Datenpunkt und Regressionsgerade. Im Fall der linearen Regression kann \\(SSE\\) analytisch minimiert werden, was z.B. bei nichtlinearen Modellen nicht der Fall ist. Analytisch finden wir das Minimum von \\(SSE\\) wo dessen partielle Ableitungen in Bezug auf die beiden Modellparameter beide Null sind: \\(\\frac{\\partial SSE}{\\partial \\beta_0}=0\\) und \\(\\frac{\\partial SSE}{\\partial \\beta_1}=0\\). Unter Anwendung der Definition von \\(SEE\\) aus Gleichung (12.2) und der Summenregel4 und der Kettenregel5, die Sie noch aus der Schule kennen werden, erhalten wir: \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_0}=-2 \\cdot \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{12.3} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_1}=-2 \\cdot \\sum_{i=1}^{n}x_i \\cdot \\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{12.4} \\end{equation}\\] Gleichungen (12.3) und (12.4) bilden ein Gleichungssystem mit zwei Gleichungen und zwei Unbekannten, das wir eindeutig lösen können. Zuerst lösen wir Gleichung (12.3) nach \\(\\beta_0\\) auf (nachdem wir durch -2 geteilt haben): \\[\\begin{equation} \\sum_{i=1}^{n}y_i-n \\cdot \\beta_0-\\beta_1 \\cdot \\sum_{i=1}^{n}x_i=0 \\tag{12.5} \\end{equation}\\] \\[\\begin{equation} n \\cdot \\beta_0=\\sum_{i=1}^{n}y_i-\\beta_1 \\cdot \\sum_{i=1}^{n}x_i \\tag{12.6} \\end{equation}\\] \\[\\begin{equation} \\beta_0=\\bar{y}-\\beta_1 \\cdot \\bar{x} \\tag{12.7} \\end{equation}\\] Formal sind das jetzt Parameterschätzer (das “Dach”-Symbol bezeichnet Schätzer): \\[\\begin{equation} \\hat\\beta_0=\\bar{y}-\\hat\\beta_1 \\cdot \\bar{x} \\tag{12.8} \\end{equation}\\] Sodann setzen wir Gleichung (12.8) in Gleichung (12.4) ein (nachdem wir durch -2 geteilt haben): \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\beta_0 \\cdot x_i-\\beta_1 \\cdot x_i^2\\right)=0 \\tag{12.9} \\end{equation}\\] \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i+\\hat\\beta_1 \\cdot \\bar{x} \\cdot x_i-\\hat\\beta_1 \\cdot x_i^2\\right)=0 \\tag{12.10} \\end{equation}\\] Schließlich lösen wir Gleichung (12.10) nach \\(\\beta_1\\) auf: \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)-\\hat\\beta_1 \\cdot \\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)=0 \\tag{12.11} \\end{equation}\\] \\[\\begin{equation} \\hat\\beta_1=\\frac{\\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)}{\\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)} \\tag{12.12} \\end{equation}\\] Über eine Reihe von Schritten, die ich hier überspringe, erhalten wir: \\[\\begin{equation} \\hat\\beta_1=\\frac{SSXY}{SSX} \\tag{12.13} \\end{equation}\\] \\(SSX=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)^2\\) ist ein Maß für die Varianz der Daten in \\(x\\)-Richtung. \\(SSXY=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right) \\cdot \\left(y_i-\\bar{y}\\right)\\) ist ein Maß für die Kovarianz der Daten. Es gibt auch \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\), das entsprechend ein Maß für die Varianz der Daten in \\(y\\)-Richtung ist. Gleichung (12.13) ist eine exakte Lösung für \\(\\hat\\beta_1\\). Wir setzen nun Gleichung (12.13) in Gleichung (12.8) ein und haben eine exakte Lösung für \\(\\hat\\beta_0\\). Berechnen wir nun die Parameter für unsere Reisedaten mit der lm() Funktion: # lineare Regression der Reisedaten reise_fit &lt;- lm(distanz ~ stationen, data = reisedat) # Informationen über geschätzte Parameterwerte ausgeben coef(summary(reise_fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.9232 1.01874 4.833 7.533e-06 ## stationen 0.6934 0.05976 11.604 4.641e-18 In der ersten Spalte (“Estimate”) dieses Outputs finden Sie die Werte der Parameterschätzer, wobei “(Intercept)” für \\(\\beta_0\\) steht und “stationen” für \\(\\beta_1\\). Anhand von \\(\\beta_1\\) können wir ablesen, dass die Entfernung zwischen zwei Stationen im Mittel 0.7km beträgt. Der Achsenabschnitt \\(\\beta_0\\) hat keine direkte Entsprechung.6 Auf die anderen Spalten werden wir weiter unten zu sprechen kommen. Plotten wir nun die so ermittelte Regressionsgerade \\(y_i=4.9+0.7\\cdot x_i+\\epsilon_i\\): # Modellanpassung plotten plot(reisedat$stationen, reisedat$distanz, pch = 19, type = &#39;p&#39;, xlab = &quot;Anzahl Stationen&quot;, ylab = &quot;Entfernung (km)&quot;) abline(coef(reise_fit), lwd = 3, col = &quot;red&quot;) 12.5 Signifikanz der Regression Nun, dass wir Werte für die Regressionsparameter haben, müssen wir uns fragen, ob diese Werte statistisch signifikant sind oder ob sie durch Zufall aus dem (angenommenen) Zufallsprozess der Stichprobenziehung entstanden sein könnten. Dazu testen wir formal, ob die vom Modell erklärte Varianz in den Daten signifikant größer als die nicht erklärte Varianz ist. Das ist ein F-Test-Problem, das wir über die sogenannte Varianzanalyse (ANOVA) angehen. ANOVA beginnt mit der Erstellung der ANOVA-Tabelle (Tabelle 12.3). Dies geschieht in R im Hintergrund und wird selten explizit betrachtet; tun wir es hier aber trotzdem, damit wir verstehen was passiert. Tabelle 12.3: ANOVA-Tabelle der linearen Regression. Varianz-quelle Quadrat-summe Freiheits-grad (\\(df\\)) Varianz F-Statistik (\\(F_s\\)) p-Wert Regression \\(SSR=\\\\SSY-SSE\\) \\(1\\) \\(\\frac{SSR}{df_{SSR}}\\) \\(\\frac{\\frac{SSR}{df_{SSR}}}{s^2}\\) \\(1-F\\left(F_s,1,n-2\\right)\\) Fehler \\(SSE\\) \\(n-2\\) \\(\\frac{SSE}{df_{SSE}}=s^2\\) Gesamt \\(SSY\\) \\(n-1\\) Schauen wir uns zunächst die zweiten Spalte der Tabelle 12.3 an: \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\) ist ein Maß für die Gesamtvarianz der Daten (in \\(y\\)-Richtung), d.h. wie stark die Datenpunkte um den Gesamtmittelwert streuen (Abbildung 12.3, links). \\(SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2\\) ist ein Maß für die Fehlervarianz, d.h. wie stark die Datenpunkte um die Regressionsgerade streuen (Abbildung 12.3, rechts). Das ist die Varianz, die nach der Modellanpassung übrig ist (“nicht erklärt”). \\(SSR=SSY-SSE\\) ist folglich ein Maß für die vom Modell erklärte Varianz. Abbildung 12.3: Variation der Datenpunkte um den Mittelwert, zusammengefasst durch \\(SSY\\) (links), und um die Regressionsgerade, zusammengefasst durch \\(SSE\\) (rechts). In der dritten Spalte der Tabelle 12.3 stehen die Freiheitsgrade der drei Varianzterme. Diese können als Anzahl der Werte in einer Stichprobe, die für die Berechnung der jeweiligen Parameter frei zur Verfügung stehen, verstanden werden (vgl. Kapitel 4): In die Berechnung von \\(SSY\\) geht \\(\\bar y\\) ein, für dessen Berechnung die Werte der Stichprobe bereits einmal verwendet wurden; dadurch ist die Anzahl Freiheitsgrade \\(df_{SSY}=n-1\\). In die Berechnung von \\(SSE\\) gehen \\(\\beta_0\\) und \\(\\beta_1\\) ein (Gleichung (12.2)), d.h. die Anzahl Freiheitsgrade ist \\(df_{SSE}=n-2\\). Für \\(SSR\\) gilt dann einfach \\(df_{SSR}=df_{SSY}-df_{SSE}=1\\). Die Freiheitsgrade werden verwendet, um die Varianzterme in der vierten Spalte der Tabelle 12.3 zu normalisieren, wobei \\(s^2\\) Fehlervarianz genannt wird. In der fünften Spalte der Tabelle 12.3 finden wir das Verhältnis von zwei Varianzen; Regressionsvarianz über Fehlervarianz. Von einer signifikanten Regression erwarten wir, dass die (durch das Modell erklärte) Regressionsvarianz viel größer ist als die (durch das Modell nicht erklärte) Fehlervarianz. Dies ist ein F-Test Problem, bei dem getestet wird, ob sich die durch das Modell erklärte Varianz signifikant von der durch das Modell nicht erklärten Varianz unterscheidet. Das Verhältnis der beiden Varianzen dient als F-Statistik \\(F_s\\). Die sechste Spalte der Tabelle 12.3 gibt dann den p-Wert des F-Tests an, d.h. die Wahrscheinlichkeit, \\(F_s\\) oder einen größeren Wert (d.h. ein noch besseres Modell) zufällig zu erhalten, wenn die Nullhypothese \\(H_0\\) wahr ist (vgl. Kapitel 10). Im Fall der linearen Regression ist \\(H_0:\\frac{SSR}{df_{SSR}}=s^2\\), d.h. die beiden Varianzen sind gleich, und \\(H_1:\\frac{SSR}{df_{SSR}}&gt;s^2\\), d.h. die erklärte Varianz ist größer als die nicht erklärte. Wie in Kapitel 10 bereits diskutiert folgt \\(F_s\\) einer F-Verteilung mit den Parametern \\(1\\) und \\(n-2\\) unter der Nullhypothese (Abbildung 12.4). Die rote Linie in Abbildung 12.4 markiert einen bestimmten Wert von \\(F_s\\) (hier zwischen 10 und 11) und den entsprechenden Wert der Verteilungsfunktion der F-Verteilung (\\(F\\left(F_s,1,n-2\\right)\\)). Der p-Wert ist \\(\\Pr\\left(Z&gt; F_s\\right)=1-F\\left(F_s,1,n-2\\right)\\) und beschreibt die Wahrscheinlichkeit, dieses oder ein größeres Varianzverhältnis zufällig (aufgrund der zufälligen Stichprobenziehung) zu erhalten, selbst wenn die beiden Varianzen tatsächlich gleich sind. Abbildung 12.4: Verteilungsfunktion der F-Verteilung der F-Statistik \\(F_s\\). Rot: Bestimmter Wert für \\(F_s\\) und entsprechender Wert der Verteilungsfunktion. Für die Regression der Reisedaten ist der p-Wert wesentlich kleiner als das konventionelle Signifikanzniveau \\(\\alpha=0.01\\), daher lehnen wir die die Nullhypothese ab und bezeichnen die Regression als statistisch signifikant. Schauen wir uns die ANOVA-Tabelle fuer das Beispiel an: anova(reise_fit) ## Analysis of Variance Table ## ## Response: distanz ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## stationen 1 3309 3309 135 &lt;2e-16 *** ## Residuals 71 1745 25 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Im Vergleich zu Tabelle 12.3 lässt R die letzte Zeile (\\(SSY\\)) weg und tauscht die Spalten “Quadratsumme” und “Freiheitsgrad”. 12.6 Konfidenzintervalle und Signifikanz der Parameter Da die Modellanpassung nicht perfekt ist, haben die Parameterschätzer Standardfehler, d.h. sie werden wie andere statistische Kennzahlen als Realisationen eines Zufallsprozesses interpretiert. Das führt uns zu Konfidenzintervallen und t-Tests auf Signifikanz der einzelnen Parameter. Der Standardfehlern für \\(\\hat\\beta_0\\) ist: \\[\\begin{equation} s_{\\hat\\beta_0}=\\sqrt{\\frac{\\sum_{i=1}^{n}x_i^2}{n} \\cdot \\frac{s^2}{SSX}} \\tag{12.14} \\end{equation}\\] Wenn wir diese Formel in ihre einzelnen Teile zerlegen, sehen wir: Je mehr Datenpunkte \\(n\\) wir haben, desto kleiner ist der Standardfehler, d.h. desto mehr Vertrauen haben wir in die Schätzung. Außerdem gilt, je größer die Variation in \\(x\\) (\\(SSX\\)), desto kleiner der Standardfehler. Beide Effekte machen intuitiv Sinn: Je mehr Datenpunkte wir haben und je mehr Ausprägungen von \\(x\\) wir abgedeckt haben, desto sicherer können wir sein, dass unsere Stichprobe repräsentativ für die Grundgesamtheit ist. Umgekehrt gilt: Je größer die Fehlervarianz \\(s^2\\), d.h. je kleiner die Erklärungskraft unseres Modells, desto größer der Standardfehler. Und je mehr \\(x\\)-Datenpunkte von Null entfernt sind, d.h. je größer \\(\\sum_{i=1}^{n}x_i^2\\), desto geringer ist unser Vertrauen in den Achsenabschnitt (wo \\(x=0\\) ist) und damit steigt der Standardfehler. Der Standardfehler für \\(\\hat\\beta_1\\) ist: \\[\\begin{equation} s_{\\hat\\beta_1}=\\sqrt{\\frac{s^2}{SSX}} \\tag{12.15} \\end{equation}\\] Hier gilt die gleiche Interpretation wie zuvor, außer dass es keinen Einfluss der Größe der \\(x\\)-Datenpunkte gibt. Wir können auch einen Standardfehler für neue Vorhersagen \\(\\hat y\\) für gegebene \\(\\hat x\\) festlegen: \\[\\begin{equation} s_{\\hat y}=\\sqrt{s^2 \\cdot \\left(\\frac{1}{n}+\\frac{\\left(\\hat x-\\bar x\\right)^2}{SSX}\\right)} \\tag{12.16} \\end{equation}\\] Dieselbe Interpretation gilt auch hier, nur dass jetzt ein zusätzlicher Term \\(\\left(\\hat x-\\bar x\\right)^2\\) auftaucht, der besagt, je weiter der neue \\(x\\)-Wert vom Zentrum der ursprünglichen Daten (den Trainings- oder Kalibrierungsdaten) entfernt ist, desto größer ist der Standardfehler der neuen Vorhersage, d.h. desto geringer ist die Zuversicht, dass sie korrekt ist. Anmerkung: Die Formeln für die Standardfehler ergeben sich aus den grundlegenden Annahmen der linearen Regression, auf die wir weiter unten eingehen werden. Die mathematische Herleitung lassen wir hier aus. Aus den Standardfehlern können wir Konfidenzintervalle für die Parameterschätzer wie folgt berechnen: \\[\\begin{equation} \\Pr\\left(\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\leq \\beta_0\\leq \\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right)=0.95 \\tag{12.17} \\end{equation}\\] Gleichung (12.17) ist das zentrale 95%-Konfidenzintervall, in dem der wahre Parameterwert, hier \\(\\beta_0\\), mit einer Wahrscheinlichkeit von 0.95 liegt. Vgl. Konfidenzintervall des Mittelwertschätzers (Kapitel 8). Wir können das Intervall auch wie folgt schreiben: \\[\\begin{equation} KI=\\left[\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0};\\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right] \\tag{12.18} \\end{equation}\\] Wie bei dem Konfidenzintervall des Mittelwertschätzers (Kapitel 8) liegt das Konfidenzintervall symmetrisch um den Parameterschätzwert \\(\\hat\\beta_0\\) und ergibt sich aus einer t-Verteilung mit dem Parameter \\(n-2\\), deren Breite durch den Standardfehler \\(s_{\\hat\\beta_0}\\) moduliert wird. Erinnern Sie sich, dass die Breite der t-Verteilung ebenfalls durch den Stichprobenumfang kontrolliert wird und mit zunehmendem \\(n\\) immer schmaler wird. Die gleichen Formeln gelten für \\(\\beta_1\\) und \\(y\\): \\[\\begin{equation} \\Pr\\left(\\hat\\beta_1-t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\leq \\beta_1\\leq \\hat\\beta_1+t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\right)=0.95 \\tag{12.19} \\end{equation}\\] \\[\\begin{equation} \\Pr\\left(\\hat y-t_{n-2;0.975} \\cdot s_{\\hat y}\\leq y\\leq \\hat y+t_{n-2;0.975} \\cdot s_{\\hat y}\\right)=0.95 \\tag{12.20} \\end{equation}\\] Die Formeln für die Konfidenzintervalle (Gleichungen (12.17), (12.19) und (12.20)) ergeben sich aus den Grundannahmen der linearen Regression (vgl. Kapitel 8): Die Residuen sind unabhängig identisch verteilt (u.i.v.) gemäß einer Normalverteilung, d.h. \\(\\epsilon_i\\sim N(0,\\sigma)\\), und das lineare Modell ist korrekt. Dann lässt sich mathematisch zeigen, dass \\(\\frac{\\hat\\beta_0-\\beta_0}{s_{\\hat\\beta_0}}\\), \\(\\frac{\\hat\\beta_1-\\beta_1}{s_{\\hat\\beta_1}}\\) und \\(\\frac{\\hat y-y}{s_{\\hat y}}\\) \\(t_{n-2}\\)-verteilt sind (t-Verteilung mit \\(n-2\\) Freiheitsgraden). Da das zentrale 95%-Konfidenzintervall einer \\(t_{n-2}\\)-verteilten Zufallsvariablen \\(Z\\) \\(\\Pr\\left(-t_{n-2;0.975}\\leq Z\\leq t_{n-2;0. 975}\\right)=0.95\\) ist (Abbildung 12.5), können wir jeden der oben genannten drei Terme für \\(Z\\) einsetzen und die Ungleichung umstellen, um zu den Gleichungen (12.17), (12.19) und (12.20) zu gelangen. Abbildung 12.5: Links: Dichtefunktion einer t-verteilten Zufallsvariablen \\(Z\\), wobei das zentrale 95%-Konfidenzintervall rot markiert ist. 95% der Dichtefunktion liegen zwischen den beiden Grenzen, 2.5% liegen links von der unteren Grenze und 2.5% rechts von der oberen Grenze. Rechts: Verteilungsfunktion der gleichen t-verteilten Zufallsvariablen \\(Z\\). Die obere Grenze des 95%-Konfidenzintervalls ist als \\(t_{n-2;0.975}\\) definiert, d.h. das 0.975-Perzentil der Verteilung, während die untere Grenze als \\(t_{n-2;0.025}\\) definiert ist, was aufgrund der Symmetrie der Verteilung \\(-t_{n-2;0.975}\\) entspricht. Die Signifikanz der Parameterschätzer wird mit Hilfe eines t-Tests ermittelt (vgl. Kapitel 9 und 10). Die Nullhypothese ist, dass die wahren Parameterwerte gleich Null sind, d.h. die Parameterschätzer nicht signifikant sind: \\[\\begin{equation} H_0:\\beta_0=0 \\tag{12.21} \\end{equation}\\] \\[\\begin{equation} H_0:\\beta_1=0 \\tag{12.22} \\end{equation}\\] Diese Hypothese wird gegen die Alternativhypothese getestet, dass die wahren Parameterwerte ungleich Null sind, d.h. dass die Parameterschätzer signifikant sind: \\[\\begin{equation} H_1:\\beta_0\\neq 0 \\tag{12.23} \\end{equation}\\] \\[\\begin{equation} H_1:\\beta_1\\neq 0 \\tag{12.24} \\end{equation}\\] Die Teststatistiken sind: \\[\\begin{equation} t_s=\\frac{\\hat\\beta_0-0}{s_{\\hat\\beta_0}}\\sim t_{n-2} \\tag{12.25} \\end{equation}\\] \\[\\begin{equation} t_s=\\frac{\\hat\\beta_1-0}{s_{\\hat\\beta_1}}\\sim t_{n-2} \\tag{12.26} \\end{equation}\\] Das “Tilde”-Symbol (\\(\\sim\\)) bedeutet, dass die Teststatistik einer bestimmten Verteilung folgt, hier der t-Verteilung. Dies ergibt sich wiederum aus den oben erwähnten Regressionsannahmen. Die Annahmen sind die gleichen wie beim üblichen t-Test der Mittelwerte (Kapitel 9 und 10), außer dass im Fall der linearen Regression die Residuen als u.i.v. normal angenommen werden, während im Fall der Mittelwerte die tatsächlichen Datenpunkte \\(y\\) als u.i.v. normal angenommen werden. Analog zum üblichen 2-seitigen t-Test ist der p-Wert definiert als: \\[\\begin{equation} 2 \\cdot \\Pr\\left(t&gt;|t_s|\\right)=2 \\cdot \\left(1-F_t\\left(|t_s|\\right)\\right) \\tag{12.27} \\end{equation}\\] Das Symbol \\(F_t\\left(|t_s|\\right)\\) bezeichnet den Wert der kumulativen Verteilungsfunktion der t-Verteilung an der Stelle des absoluten Wertes der Teststatistik (\\(|t_s|\\), Abbildung 12.6). Mit einem Signifikanzniveau von z.B. \\(\\alpha=0.01\\) gelangen wir zu einem kritischen Wert der Teststatistik \\(t_c=t_{n-2;0.995}\\), bei dessen Überschreitung wir die Nullhypothese ablehnen und die Parameterschätzer als signifikant bezeichnen (Abbildung 12.6). Abbildung 12.6: Schema des t-Tests auf Signifikanz der Parameterschätzer. Die Teststatistik folgt einer t-Verteilung unter der Nullhypothese. Der tatsächliche Wert der Teststatistik \\(t_s\\) ist blau markiert und wird für den 2-seitigen Test bei Null gespiegelt. Der kritische Wert der Teststatistik \\(t_c\\), den wir von einem Signifikanzniveau von \\(\\alpha=0.01\\) erhalten, ist rot markiert; auch dieser wird für den 2-seitigen Test gespiegelt. Wir lehnen die Nullhypothese ab, wenn \\(|t_s|&gt;t_c\\), d.h. für Werte von \\(t_s\\) kleiner als \\(-t_c\\) und größer als \\(t_c\\), und nennen diese Parameterschätzer dann signifikant. Wir behalten die Nullhypothese bei wenn \\(|t_s|\\leq t_c\\), d.h. für Werte von \\(t_s\\) zwischen \\(-t_c\\) und \\(t_c\\), und nennen diesen Parameterschätzer dann (vorläufig) nicht signifikant. In dem gezeigten Beispiel ist der Parameterschätzer nicht signifikant. Jetzt verstehen wir auch die restlichen Informationen des Outputs der lm() Funktion (s. oben): coef(summary(reise_fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.9232 1.01874 4.833 7.533e-06 ## stationen 0.6934 0.05976 11.604 4.641e-18 Wie bereits oben erwähnt steht die Zeile “(Intercept)” für \\(\\beta_0\\) und die Zeile “stationen” für \\(\\beta_1\\). In Spalte “Estimate” stehen die Werte der Parameterschätzer. In Spalte “Std. Error” stehen deren Standardfehler (Gleichungen (12.14) und (12.15)). In Spalte “t value” stehen die entsprechenden Werte der Teststatistik (Gleichungen (12.25) und (12.26)). In Spalte “Pr(&gt;|t|)” stehen die p-Werte der t-Tests auf Signifikanz der Parameter (Gleichung (12.27)). Wir sehen, dass in unserem Beispiel beide Parameter signifikant sind (die p-Werte sind wesentlich kleiner als das konventionelle \\(\\alpha=0.01\\)). 12.7 Güte der Modellanpassung Die Signifikanz der Parameter der Regression ist eine Sache. Wie gut aber ist das Modell im Beschreiben der Daten? D.h. wieviel von der Varianz in den Daten wird vom Modell erklärt? Die Güte der Modellanpassung kann in erster Linie mit dem Bestimmtheitsmaß (\\(r^2\\)) begutachtet werden, welches als Anteil der Varianz (in \\(y\\)-Richtung) definiert ist, der durch das Modell erklärt wird: \\[\\begin{equation} r^2=\\frac{SSY-SSE}{SSY}=1-\\frac{SSE}{SSY} \\tag{12.28} \\end{equation}\\] Das Bestimmtheitsmaß ist der Korrelationskoeffizient nach Bravais-Pearson zum Quadrat (vgl. Kapitel 5). Wie wir an Gleichung (12.28) sehen, wenn das Modell nicht mehr Variation als die Gesamtvariation um den Mittelwert erklärt, d.h. \\(SSE=SSY\\) ist, dann ist \\(r^2=0\\). Umgekehrt, wenn das Modell perfekt zu den Daten passt, d.h. \\(SSE=0\\) ist, dann ist \\(r^2=1\\). Werte dazwischen stellen unterschiedliche Grade der Anpassungsgüte dar. Das kann wiederum mit Abbildung 12.3 veranschaulicht werden, wobei der linke Teil \\(SSY\\) und der rechte Teil \\(SSE\\) verdeutlicht. Wenn es darum geht, Modelle unterschiedlicher Komplexität (d.h. mit mehr oder weniger Parametern) mit \\(r^2\\) zu vergleichen, dann ist es sinnvoll, das Bestimmtheitsmaß mit der Anzahl der Modellparameter zu korrigieren, da komplexere Modelle (mehr Parameter) automatisch zu besseren Anpassungen führen, einfach aufgrund der größeren Freiheitsgrade, die komplexere Modelle bei der Anpassung der Daten haben. Dies führt zum korregierten \\(r^2\\): \\[\\begin{equation} \\bar r^2=1-\\frac{\\frac{SSE}{df_{SSE}}}{\\frac{SSY}{df_{SSY}}}=1-\\frac{SSE}{SSY} \\cdot \\frac{df_{SSY}}{df_{SSE}} \\tag{12.29} \\end{equation}\\] Rechnen wir \\(r^2\\) und \\(\\bar r^2\\) für die Regression der Reisedaten aus: summary(reise_fit)$r.squared ## [1] 0.6548 summary(reise_fit)$adj.r.squared ## [1] 0.6499 D.h. rund 65% der Varianz in den Entfernungs-Daten wird durch das lineare Modell mit Anzahl Stationen als Prädiktor erklärt. Aber was heißt Güte der Modellanpassung? Sind alle Modellannahmen erfüllt? Folgende Annahmen ergeben sich aus der Maximum-Likelihood-Theorie (vgl. Schätzen von Verteilungsparametern, Kapitel 8): Die Residuen sind unabhängig, in diesem Fall gibt es keine serielle Korrelation in der Residuengrafik - dies kann mit dem Durbin-Watson-Test getestet werden Die Residuen sind normalverteilt - dies kann visuell mit Hilfe des Quantil-Quantil-Diagramms (QQ-Plot) und dem Residuen-Histogramm beurteilt werden, und kann mit dem Kolmogorov-Smirnov-Test (Kapitel 11) und dem Shapiro-Wilk-Test getestet werden Die Varianz ist für alle Residuen konstant (die Residuen sind homoskedastisch), d.h. es erfolgt kein “Auffächern” der Residuen Sind diese Annahmen nicht erfüllt, können wir auf Datentransformation, gewichtete Regression oder Generalisierte Lineare Modelle zurückgreifen. Letzteres ist wird im Master Global Change Geography unterrichtet. Eine erste nützliche diagnostische Darstellung ist die der Residuen in Serie, d.h. nach Index \\(i\\), um zu sehen, ob es ein Muster aufgrund des Datenerfassungsprozesses gibt. Für unser Beispiel: # Residuen gegen Index plotten plot(residuals(reise_fit), pch = 19, type = &#39;p&#39;, ylim = c(-25,25)) abline(h = 0, lwd = 3, col = &quot;red&quot;) Dieser Datensatz zeigt kein erkennbares Muster, was für eine Unabhängigkeit der Residuen spricht. Wir sollten auch die Residuen nach dem modellierten Wert von \\(y\\) plotten, um zu sehen, ob es ein Muster als Funktion der Größenordnung von \\(y\\) gibt: # Residuen gegen modellierte Werte von y plotten plot(fitted.values(reise_fit),residuals(reise_fit), pch = 19, type = &#39;p&#39;, ylim = c(-25,25)) abline(h = 0, lwd = 3, col = &quot;red&quot;) Diese Grafik zeigt eine leichte Zunahme der Residuenvarianz von links nach rechts, d.h. in Richtung größerer Werte - eine Form der Heteroskedastizität. Die Annahme, dass die Residuen normalverteilt sind, kann anhand des QQ-Plots beurteilt werden: qqnorm(residuals(reise_fit)) qqline(residuals(reise_fit)) Im QQ-Plot stellt jeder Datenpunkt ein bestimmtes Quantil der empirischen Verteilung dar. Dieses Quantil (nach Standardisierung) wird gegen den Wert dieses Quantils geplotted (vertikale Achse), der unter einer Standard-Normalverteilung (horizontale Achse) erwartet wird. Die resultierenden Formen sagen etwas über die Verteilung der Residuen aus. Im Fall einer Normalverteilung zum Beispiel, fallen alle Residuen auf eine gerade Linie (vgl. Kapitel 8). In unserem Beispiel deutet der QQ-Plot darauf hin, dass die Flanken der Residuenverteilung flacher als bei einer Normalverteilung abfallen, vermutlich auf aufgrund der o.g. Heteroskedastizität. Das können wir beim Histogramm der Residuen nicht so gut sehen da die Vergleichverteilung fehlt: # Histogramm der Residuen plotten hist(residuals(reise_fit), xlim = c(-25,25)) Die Heteroskedastizität würde man angehen, indem man mit einem Generalisierten Linearen Modell eine andere Residuenverteilung als die Normalverteilung annimmt. Das führt aber im Rahmen dieses Kurses zu weit. Es bleibt somit festzuhalten, dass das Regressionsproblem mit den hier behandelten Methoden noch nicht vollständig gelöst ist. Literatur "],
["13-refs.html", "Literatur", " Literatur "]
]
