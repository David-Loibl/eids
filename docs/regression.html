<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Kapitel 12 Lineare Regression | Einführung in die Statistik</title>
  <meta name="description" content="This is the script of the course ‘Einführung in die Statistik’ (in German) run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Kapitel 12 Lineare Regression | Einführung in die Statistik" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the script of the course ‘Einführung in die Statistik’ (in German) run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="github-repo" content="krueger-t/eids" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Kapitel 12 Lineare Regression | Einführung in die Statistik" />
  
  <meta name="twitter:description" content="This is the script of the course ‘Einführung in die Statistik’ (in German) run at the Geography Department of Humboldt-Universität zu Berlin." />
  

<meta name="author" content="Tobias Krueger" />


<meta name="date" content="2020-11-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chi2testkstest.html"/>
<link rel="next" href="literatur.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Einfuehrung in die Statistik</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Vorwort</a></li>
<li class="chapter" data-level="1" data-path="einfuehrung.html"><a href="einfuehrung.html"><i class="fa fa-check"></i><b>1</b> Einführung</a><ul>
<li class="chapter" data-level="1.1" data-path="einfuehrung.html"><a href="einfuehrung.html#statistik-im-empirischen-forschungsprozess"><i class="fa fa-check"></i><b>1.1</b> Statistik im empirischen Forschungsprozess</a></li>
<li class="chapter" data-level="1.2" data-path="einfuehrung.html"><a href="einfuehrung.html#warum-statistik"><i class="fa fa-check"></i><b>1.2</b> Warum Statistik?</a></li>
<li class="chapter" data-level="1.3" data-path="einfuehrung.html"><a href="einfuehrung.html#orga"><i class="fa fa-check"></i><b>1.3</b> Organisatorisches</a></li>
<li class="chapter" data-level="1.4" data-path="einfuehrung.html"><a href="einfuehrung.html#mathematische-notation-und-grundlagen"><i class="fa fa-check"></i><b>1.4</b> Mathematische Notation und Grundlagen</a><ul>
<li class="chapter" data-level="1.4.1" data-path="einfuehrung.html"><a href="einfuehrung.html#exponential--und-logarithmusfunktion"><i class="fa fa-check"></i><b>1.4.1</b> Exponential- und Logarithmusfunktion</a></li>
<li class="chapter" data-level="1.4.2" data-path="einfuehrung.html"><a href="einfuehrung.html#quadratische-funktion-und-wurzelfunktion"><i class="fa fa-check"></i><b>1.4.2</b> Quadratische Funktion und Wurzelfunktion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="begriffe.html"><a href="begriffe.html"><i class="fa fa-check"></i><b>2</b> Grundbegriffe und Datenerhebung</a><ul>
<li class="chapter" data-level="2.1" data-path="begriffe.html"><a href="begriffe.html#statistische-grundbegriffe"><i class="fa fa-check"></i><b>2.1</b> Statistische Grundbegriffe</a></li>
<li class="chapter" data-level="2.2" data-path="begriffe.html"><a href="begriffe.html#datenerhebung"><i class="fa fa-check"></i><b>2.2</b> Datenerhebung</a></li>
<li class="chapter" data-level="2.3" data-path="begriffe.html"><a href="begriffe.html#skalenniveaus"><i class="fa fa-check"></i><b>2.3</b> Skalenniveaus</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="haeufigkeit.html"><a href="haeufigkeit.html"><i class="fa fa-check"></i><b>3</b> Häufigkeiten und Lageparameter</a><ul>
<li class="chapter" data-level="3.1" data-path="haeufigkeit.html"><a href="haeufigkeit.html#ziel-der-deskriptiven-statistik"><i class="fa fa-check"></i><b>3.1</b> Ziel der deskriptiven Statistik</a></li>
<li class="chapter" data-level="3.2" data-path="haeufigkeit.html"><a href="haeufigkeit.html#häufigkeiten"><i class="fa fa-check"></i><b>3.2</b> Häufigkeiten</a></li>
<li class="chapter" data-level="3.3" data-path="haeufigkeit.html"><a href="haeufigkeit.html#lageparameter"><i class="fa fa-check"></i><b>3.3</b> Lageparameter</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="streuung.html"><a href="streuung.html"><i class="fa fa-check"></i><b>4</b> Streuungsparameter, Schiefe und Wölbung</a><ul>
<li class="chapter" data-level="4.1" data-path="streuung.html"><a href="streuung.html#streuungsparameter"><i class="fa fa-check"></i><b>4.1</b> Streuungsparameter</a></li>
<li class="chapter" data-level="4.2" data-path="streuung.html"><a href="streuung.html#schiefe-und-wölbung-von-häufigkeitsverteilungen"><i class="fa fa-check"></i><b>4.2</b> Schiefe und Wölbung von Häufigkeitsverteilungen</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="korrelation.html"><a href="korrelation.html"><i class="fa fa-check"></i><b>5</b> Korrelationsanalyse</a><ul>
<li class="chapter" data-level="5.1" data-path="korrelation.html"><a href="korrelation.html#nominalskalierte-merkmale-kontingenztabelle-und-chi-quadrat-statistik"><i class="fa fa-check"></i><b>5.1</b> Nominalskalierte Merkmale: Kontingenztabelle und Chi-Quadrat Statistik</a></li>
<li class="chapter" data-level="5.2" data-path="korrelation.html"><a href="korrelation.html#ordinalskalierte-merkmale-rangkorrelationskoeffizient-nach-spearman"><i class="fa fa-check"></i><b>5.2</b> Ordinalskalierte Merkmale: Rangkorrelationskoeffizient nach Spearman</a></li>
<li class="chapter" data-level="5.3" data-path="korrelation.html"><a href="korrelation.html#metrische-merkmale-scatterplot-streudiagramm-und-korrelationskoeffizient-nach-bravais-pearson"><i class="fa fa-check"></i><b>5.3</b> Metrische Merkmale: Scatterplot (Streudiagramm) und Korrelationskoeffizient nach Bravais-Pearson</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="wahrscheinlichkeit.html"><a href="wahrscheinlichkeit.html"><i class="fa fa-check"></i><b>6</b> Grundlagen der Wahrscheinlichkeitsrechnung</a></li>
<li class="chapter" data-level="7" data-path="verteilungen.html"><a href="verteilungen.html"><i class="fa fa-check"></i><b>7</b> Verteilungen</a></li>
<li class="chapter" data-level="8" data-path="schaetzen.html"><a href="schaetzen.html"><i class="fa fa-check"></i><b>8</b> Schaetzen von Verteilungsparametern</a></li>
<li class="chapter" data-level="9" data-path="ttest.html"><a href="ttest.html"><i class="fa fa-check"></i><b>9</b> T-Test</a></li>
<li class="chapter" data-level="10" data-path="ftest.html"><a href="ftest.html"><i class="fa fa-check"></i><b>10</b> F-Test</a></li>
<li class="chapter" data-level="11" data-path="chi2testkstest.html"><a href="chi2testkstest.html"><i class="fa fa-check"></i><b>11</b> Chi2- und Kolmogorow-Smirnow-Test</a></li>
<li class="chapter" data-level="12" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>12</b> Lineare Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="regression.html"><a href="regression.html#definitionen"><i class="fa fa-check"></i><b>12.1</b> Definitionen</a></li>
<li class="chapter" data-level="12.2" data-path="regression.html"><a href="regression.html#beschreibung-vs.-vorhersage"><i class="fa fa-check"></i><b>12.2</b> Beschreibung vs. Vorhersage</a></li>
<li class="chapter" data-level="12.3" data-path="regression.html"><a href="regression.html#ausblick-weiterführende-lineare-modelle"><i class="fa fa-check"></i><b>12.3</b> Ausblick: weiterführende lineare Modelle</a></li>
<li class="chapter" data-level="12.4" data-path="regression.html"><a href="regression.html#lineare-regression"><i class="fa fa-check"></i><b>12.4</b> Lineare Regression</a></li>
<li class="chapter" data-level="12.5" data-path="regression.html"><a href="regression.html#signifikanz-der-regression"><i class="fa fa-check"></i><b>12.5</b> Signifikanz der Regression</a></li>
<li class="chapter" data-level="12.6" data-path="regression.html"><a href="regression.html#konfidenzintervalle-und-signifikanz-der-parameter"><i class="fa fa-check"></i><b>12.6</b> Konfidenzintervalle und Signifikanz der Parameter</a></li>
<li class="chapter" data-level="12.7" data-path="regression.html"><a href="regression.html#güte-der-modellanpassung"><i class="fa fa-check"></i><b>12.7</b> Güte der Modellanpassung</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="literatur.html"><a href="literatur.html"><i class="fa fa-check"></i>Literatur</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Einführung in die Statistik</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Kapitel 12</span> Lineare Regression</h1>
<p>Die Fragen, die wir mit linearer Regression beantworten möchten, sind von der in Abbildung <a href="regression.html#fig:abb12-1">12.1</a> dargestellten Art: <em>Kann man Ihre Reisezeit mit der Entfernung zu Ihrem Wohnort statistisch vorhersagen?</em> (vgl. Kapitel <a href="korrelation.html#korrelation">5</a>)</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="regression.html#cb57-1"></a><span class="kw">library</span>(<span class="st">&quot;readxl&quot;</span>)</span>
<span id="cb57-2"><a href="regression.html#cb57-2"></a>dat &lt;-<span class="st"> </span><span class="kw">read_excel</span>(<span class="st">&quot;data/Reisedaten.xlsx&quot;</span>, <span class="dt">na =</span> <span class="st">&quot;-999&quot;</span>)</span>
<span id="cb57-3"><a href="regression.html#cb57-3"></a>dat &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(dat)</span>
<span id="cb57-4"><a href="regression.html#cb57-4"></a><span class="kw">names</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;x&#39;</span>,<span class="st">&#39;t&#39;</span>)</span>
<span id="cb57-5"><a href="regression.html#cb57-5"></a><span class="kw">plot</span>(dat<span class="op">$</span>x<span class="op">/</span><span class="dv">1000</span>, dat<span class="op">$</span>t, <span class="dt">xlab=</span><span class="st">&quot;Entfernung (km)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Reisezeit (min)&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:abb12-1"></span>
<img src="eids_files/figure-html/abb12-1-1.png" alt="Reisezeit in Abhängigkeit zur Entfernung. Korrelationskoeffizient nach Bravais-Pearson (Kapitel \@ref(korrelation)): 0.86." width="672" />
<p class="caption">
Abbildung 12.1: Reisezeit in Abhängigkeit zur Entfernung. Korrelationskoeffizient nach Bravais-Pearson (Kapitel <a href="korrelation.html#korrelation">5</a>): 0.86.
</p>
</div>
<p>Das <strong>Ziel</strong> ist, eine Gerade durch die Punktwolke zu legen, die den <em>Trend</em> beschreibt, so dass der Abstand der Punkte von der Geraden minimal ist.</p>
<p>Es geht um <em>2 Variablen</em> (Merkmale):</p>
<ul>
<li>die <strong>abhängige Variable</strong> <span class="math inline">\(y\)</span> (im Bsp. Reisezeit)</li>
<li>die <strong>unabhängige Variable</strong> <span class="math inline">\(x\)</span> (im Bsp. Entfernung)</li>
</ul>
<p>Die Variablen müssen <em>metrisch</em> skaliert sein. Wir wollen das generelle Verhalten von <span class="math inline">\(y\)</span> mit <span class="math inline">\(x\)</span> beschreiben. Eine Gerade stellt dabei das einfachste lineare Modell dar.</p>
<div id="definitionen" class="section level2">
<h2><span class="header-section-number">12.1</span> Definitionen</h2>
<p>Im Falle einer einzigen unabhängigen Variable lautet die Gleichung des <strong>linearen Models</strong>:</p>
<p><span class="math display" id="eq:linmodsingle">\[\begin{equation}
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i \quad \text{mit} \quad i=1,2,\ldots,n
\tag{12.1}
\end{equation}\]</span></p>
<p><span class="math inline">\(y_i\)</span> bezeichnet den Wert der <strong>abhängigen Variable</strong> für Datenpunkt <span class="math inline">\(i\)</span>, und <span class="math inline">\(x_i\)</span> den Wert der <strong>unabhängigen Variable</strong> für Datenpunkt <span class="math inline">\(i\)</span>. Der Parameter <span class="math inline">\(\beta_0\)</span> beschreibt den <strong>Achsenabschnitt</strong> der Geraden, also der Punkt an dem die Gerade die y-Achse schneidet. Der Parameter <span class="math inline">\(\beta_1\)</span> beschreibt die <strong>Steigung</strong> der Geraden. <span class="math inline">\(\epsilon_i\)</span> schließlich stellt das <strong>Residuum</strong> (also den Fehler) für Datenpunkt <span class="math inline">\(i\)</span> dar (Abbildung <a href="regression.html#fig:abb12-2">12.2</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:abb12-2"></span>
<img src="figs/abb12-2.png" alt="Lineare Regression: Definitionen."  />
<p class="caption">
Abbildung 12.2: Lineare Regression: Definitionen.
</p>
</div>
</div>
<div id="beschreibung-vs.-vorhersage" class="section level2">
<h2><span class="header-section-number">12.2</span> Beschreibung vs. Vorhersage</h2>
<p>Der primäre Zweck einer Regressionsanalyse ist die Beschreibung (oder Erklärung) der Daten im Sinne einer allgemeinen Beziehung, die sich auf die Grundgesamtheit übertragen lässt, aus der diese Daten entnommen wurden. Da diese Beziehung eine Eigenschaft der Grundgesamtheit ist, sollte diese demnach auch Vorhersagen ermöglichen. Hierbei ist jedoch Vorsicht geboten. Betrachten Sie den Zusammenhang von Jahr und Weltrekordzeit für die in Abbildung <a href="regression.html#fig:mile">12.3</a> dargestellten Daten (Meile, Herren). Wenn, wie hier, die Zeit als Prädiktor dient, wird die Regression zu einer Form der Trendanalyse, die in diesem Fall eine Verringerung der Rekordzeit mit den Jahren anzeigt.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="regression.html#cb58-1"></a><span class="co"># Daten laden</span></span>
<span id="cb58-2"><a href="regression.html#cb58-2"></a>mile &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mile/data/mile.csv&quot;</span>, <span class="dt">header=</span><span class="ot">TRUE</span>)</span>
<span id="cb58-3"><a href="regression.html#cb58-3"></a><span class="co"># lineares Modell an Daten aus 1. Hälfte des 20. JH. anpassen</span></span>
<span id="cb58-4"><a href="regression.html#cb58-4"></a>fit1 &lt;-<span class="st"> </span><span class="kw">lm</span>(seconds <span class="op">~</span><span class="st"> </span>year, <span class="dt">data =</span> mile[mile<span class="op">$</span>year<span class="op">&lt;</span><span class="dv">1950</span>,])</span>
<span id="cb58-5"><a href="regression.html#cb58-5"></a><span class="co"># Informationen zu Parameterschätzern extrahieren</span></span>
<span id="cb58-6"><a href="regression.html#cb58-6"></a><span class="kw">coef</span>(<span class="kw">summary</span>(fit1))</span></code></pre></div>
<pre><code>##                Estimate  Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 912.2339944 67.90139506 13.434687 3.614623e-08
## year         -0.3438721  0.03509494 -9.798337 9.058700e-07</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="regression.html#cb60-1"></a><span class="co"># lineares Modell an kompletten Datensatz anpassen</span></span>
<span id="cb60-2"><a href="regression.html#cb60-2"></a>fit2 &lt;-<span class="st"> </span><span class="kw">lm</span>(seconds <span class="op">~</span><span class="st"> </span>year, <span class="dt">data =</span> mile)</span>
<span id="cb60-3"><a href="regression.html#cb60-3"></a><span class="kw">coef</span>(<span class="kw">summary</span>(fit2))</span></code></pre></div>
<pre><code>##                 Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 1006.8760057 21.5319332  46.76199 1.360809e-29
## year          -0.3930488  0.0109992 -35.73431 3.779773e-26</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="regression.html#cb62-1"></a><span class="co"># Modellanpassung für 1. Hälfte des 20. JH. plotten</span></span>
<span id="cb62-2"><a href="regression.html#cb62-2"></a><span class="kw">plot</span>(mile<span class="op">$</span>year[mile<span class="op">$</span>year<span class="op">&lt;</span><span class="dv">1950</span>], mile<span class="op">$</span>seconds[mile<span class="op">$</span>year<span class="op">&lt;</span><span class="dv">1950</span>],</span>
<span id="cb62-3"><a href="regression.html#cb62-3"></a>     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1900</span>, <span class="dv">2000</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">260</span>),</span>
<span id="cb62-4"><a href="regression.html#cb62-4"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb62-5"><a href="regression.html#cb62-5"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Jahr&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Weltrekord, Meile, Herren (Sekunden)&quot;</span>)</span>
<span id="cb62-6"><a href="regression.html#cb62-6"></a><span class="kw">abline</span>(<span class="kw">coef</span>(fit1), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb62-7"><a href="regression.html#cb62-7"></a><span class="co"># Extrapolation für 2. Hälfte des 20. JH. plotten</span></span>
<span id="cb62-8"><a href="regression.html#cb62-8"></a><span class="kw">plot</span>(mile<span class="op">$</span>year, mile<span class="op">$</span>seconds,</span>
<span id="cb62-9"><a href="regression.html#cb62-9"></a>     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1900</span>, <span class="dv">2000</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">260</span>),</span>
<span id="cb62-10"><a href="regression.html#cb62-10"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb62-11"><a href="regression.html#cb62-11"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Jahr&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Weltrekord, Meile, Herren (Sekunden)&quot;</span>)</span>
<span id="cb62-12"><a href="regression.html#cb62-12"></a><span class="kw">abline</span>(<span class="kw">coef</span>(fit1), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb62-13"><a href="regression.html#cb62-13"></a><span class="co"># Modellanpassung für Gesamtdaten bis 2050 plotten</span></span>
<span id="cb62-14"><a href="regression.html#cb62-14"></a><span class="kw">plot</span>(mile<span class="op">$</span>year, mile<span class="op">$</span>seconds,</span>
<span id="cb62-15"><a href="regression.html#cb62-15"></a>     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">1900</span>, <span class="dv">2050</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">260</span>),</span>
<span id="cb62-16"><a href="regression.html#cb62-16"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb62-17"><a href="regression.html#cb62-17"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Jahr&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Weltrekord, Meile, Herren (Sekunden)&quot;</span>)</span>
<span id="cb62-18"><a href="regression.html#cb62-18"></a><span class="kw">abline</span>(<span class="kw">coef</span>(fit2), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mile"></span>
<img src="eids_files/figure-html/mile-1.png" alt="Links: Trend des Weltrekords &quot;Meile, Herren&quot; in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends auf die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: @wainer2009" width="33%" /><img src="eids_files/figure-html/mile-2.png" alt="Links: Trend des Weltrekords &quot;Meile, Herren&quot; in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends auf die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: @wainer2009" width="33%" /><img src="eids_files/figure-html/mile-3.png" alt="Links: Trend des Weltrekords &quot;Meile, Herren&quot; in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends auf die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: @wainer2009" width="33%" />
<p class="caption">
Abbildung 12.3: Links: Trend des Weltrekords “Meile, Herren” in der ersten Hälfte des 20. Jahrhunderts (Beschreibung). Mitte: Extrapolation des Trends auf die zweite Hälfte des 20. Jahrhunderts (Vorhersage). Rechts: Extrapolation des Trends bis zum Jahr 2050 (längere Vorhersage). Nach: <span class="citation">Wainer (<a href="#ref-wainer2009" role="doc-biblioref">2009</a>)</span>
</p>
</div>
<p>Der Weltrekord verbesserte sich in der ersten Hälfte des 20. Jahrhunderts linear (Abbildung <a href="regression.html#fig:mile">12.3</a>, links). Dieser Trend passt auch für die zweite Hälfte des 20. Jahrhunderts bemerkenswert gut (Abbildung <a href="regression.html#fig:mile">12.3</a>, Mitte). Wie lange kann sich der Weltrekord jedoch noch mit der gleichen Rate verbessern (Abbildung <a href="regression.html#fig:mile">12.3</a>, rechts)?</p>
<p>Dieses Beispiel zeigt deutlich die Anwendbarkeit von Regressionen für Vorhersagen innerhalb bestimmter Grenzen, zeigt jedoch gleichzeitig die Grenzen dieser einfachen Modelle für längere Vorhersagen (z.B. in Zeit und Raum). Im Falle des Weltrekords würden wir erwarten, dass die Verbesserungsrate mit der Zeit abnimmt, d.h. dass die Kurve abflacht, was ein nichtlineares Modell erfordert.</p>
</div>
<div id="ausblick-weiterführende-lineare-modelle" class="section level2">
<h2><span class="header-section-number">12.3</span> Ausblick: weiterführende lineare Modelle</h2>
<p>Wenn wir über das lineare Modell sprechen, ist die abhängige Variable immer metrisch skaliert, während die unabhängigen Variablen metrisch, nominal/ordinal oder gemischt sein können. Im Prinzip kann jede dieser Varianten mathematisch gleich behandelt werden, d.h. alle können z.B. mit der <code>lm()</code> Funktion in <em>R</em> analysiert werden. Allerdings haben sich historisch gesehen unterschiedliche Bezeichnungen für diese Varianten etabliert, die hier erwähnt werden sollen, um Verwirrung zu vermeiden (Tabellen <a href="regression.html#tab:varianten1">12.1</a> und <a href="regression.html#tab:varianten2">12.2</a>).</p>
<table>
<caption><span id="tab:varianten1">Tabelle 12.1: </span> Historische Namen für die Varianten des linearen Modells, je nachdem, ob die unabhängigen Variablen metrisch, nominal/ordinal oder gemischt sind. Die abhängige Variable ist immer metrisch skaliert.</caption>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">unabhängige Variable(n)<br>metrisch</th>
<th align="center">unabhängige Variable(n)<br>nominal/ordinal</th>
<th align="center">unabhängige Variable(n)<br>gemischt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Regression</td>
<td align="center">Varianzanalyse<br>(ANOVA)</td>
<td align="center">Kovarianzanalyse<br>(ANCOVA)</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:varianten2">Tabelle 12.2: </span> Historische Namen für Regression, je nachdem, ob wir eine oder mehrere unabhängige Variablen und eine oder mehrere abhängige Variablen haben.</caption>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">1 unabhängige Variable</th>
<th align="center">&gt;1 unabhängige Variable</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>1 abhängige Variable</strong></td>
<td align="center">Regression</td>
<td align="center">Multiple Regression</td>
</tr>
<tr class="even">
<td align="center"><strong>&gt;1 abhängige Variable</strong></td>
<td align="center">Multivariate Regression</td>
<td align="center">Multivariate multiple Regression</td>
</tr>
</tbody>
</table>
</div>
<div id="lineare-regression" class="section level2">
<h2><span class="header-section-number">12.4</span> Lineare Regression</h2>
<p>Wie soll nun die Gerade durch die Punktwolke gelegt werden, d.h. welche Werte sollen Achsenabschnitt <span class="math inline">\(\beta_0\)</span> und Steigung <span class="math inline">\(\beta_1\)</span> annehmen?</p>
<p>Typischerweise werden Regressionsprobleme gelöst, d.h. die Geraden in den Abbildungen <a href="regression.html#fig:abb12-1">12.1</a> und <a href="regression.html#fig:abb12-2">12.2</a> an die Daten angepasst, indem die Summe der quadratischen Abweichungen zwischen der Regressionsgeraden und den Datenpunkten minimiert wird - die sogenannte <strong>Kleinste-Quadrate-Schätzung</strong>.</p>
<p>Die Summe der quadratischen Abweichungen wird auch als <span class="math inline">\(SSE\)</span> bezeichnet (Sum of Squared Errors). Grafisch gesehen probieren wir in Abbildung <a href="regression.html#fig:abb12-1">12.1</a> verschiedene Geraden mit unterschiedlichen Achsenabschnitten <span class="math inline">\(\beta_0\)</span> und Steigungen <span class="math inline">\(\beta_1\)</span> aus und wählen diejenige, bei der die Summe aller vertikalen Abstände <span class="math inline">\(\epsilon_i\)</span> zum Quadrat am kleinsten ist. Mathematisch ist <span class="math inline">\(SSE\)</span> definiert als:</p>
<p><span class="math display" id="eq:sse">\[\begin{equation}
SSE=\sum_{i=1}^{n}\left(\epsilon_i\right)^2=\sum_{i=1}^{n}\left(y_i-\left(\beta_0+\beta_1 \cdot x_i\right)\right)^2
\tag{12.2}
\end{equation}\]</span></p>
<p>Das Residuum <span class="math inline">\(\epsilon_i\)</span> ist also gleich <span class="math inline">\(y_i-\left(\beta_0+\beta_1 \cdot x_i\right)\)</span>, dem vertikalen Abstand zwischen Datenpunkt und Regressionsgerade.</p>
<p>Im Falle der linearen Regression kann <span class="math inline">\(SSE\)</span> analytisch minimiert werden, was z.B. bei nichtlinearen Modellen nicht der Fall ist. Analytisch finden wir das Minimum von <span class="math inline">\(SSE\)</span> wo dessen partielle Ableitungen in Bezug auf die beiden Modellparameter beide Null sind: <span class="math inline">\(\frac{\partial SSE}{\partial \beta_0}=0\)</span> und <span class="math inline">\(\frac{\partial SSE}{\partial \beta_1}=0\)</span>. Unter Anwendung der Definition von <span class="math inline">\(SEE\)</span> aus Gleichung <a href="regression.html#eq:sse">(12.2)</a> und der <strong>Summenregel</strong> (wenn <span class="math inline">\(y=u(t) \pm v(t)\)</span> dann <span class="math inline">\(\frac{dy}{dt}=\frac{du}{dt} \pm \frac{dv}{dt}\)</span>) und der <strong>Kettenregel</strong> (wenn <span class="math inline">\(y=u(t) \pm v(t)\)</span> dann <span class="math inline">\(\frac{dy}{dt}=\frac{du}{dt} \pm \frac{dv}{dt}\)</span>) erhalten wir:</p>
<p><span class="math display" id="eq:sseb0">\[\begin{equation}
\frac{\partial SSE}{\partial \beta_0}=-2 \cdot \sum_{i=1}^{n}\left(y_i-\beta_0-\beta_1 \cdot x_i\right)=0
\tag{12.3}
\end{equation}\]</span>
<span class="math display" id="eq:sseb1">\[\begin{equation}
\frac{\partial SSE}{\partial \beta_1}=-2 \cdot \sum_{i=1}^{n}x_i \cdot \left(y_i-\beta_0-\beta_1 \cdot x_i\right)=0
\tag{12.4}
\end{equation}\]</span></p>
<p>Gleichungen <a href="regression.html#eq:sseb0">(12.3)</a> und <a href="regression.html#eq:sseb1">(12.4)</a> bilden ein Gleichungssystem mit zwei Gleichungen und zwei Unbekannten, das wir eindeutig lösen können:</p>
<p>Zuerst lösen wir Gleichung <a href="regression.html#eq:sseb0">(12.3)</a> nach <span class="math inline">\(\beta_0\)</span> auf (nachdem wir durch -2 geteilt haben):</p>
<p><span class="math display" id="eq:b01">\[\begin{equation}
\sum_{i=1}^{n}y_i-n \cdot \beta_0-\beta_1 \cdot \sum_{i=1}^{n}x_i=0
\tag{12.5}
\end{equation}\]</span>
<span class="math display" id="eq:b02">\[\begin{equation}
n \cdot \beta_0=\sum_{i=1}^{n}y_i-\beta_1 \cdot \sum_{i=1}^{n}x_i
\tag{12.6}
\end{equation}\]</span>
<span class="math display" id="eq:b03">\[\begin{equation}
\beta_0=\bar{y}-\beta_1 \cdot \bar{x}
\tag{12.7}
\end{equation}\]</span></p>
<p>Formal sind das jetzt Parameter<em>schätzer</em>:
<span class="math display" id="eq:b04">\[\begin{equation}
\hat\beta_0=\bar{y}-\hat\beta_1 \cdot \bar{x}
\tag{12.8}
\end{equation}\]</span>
Das “Dach”-Symbol bezeichnet Schätzer.</p>
<p>Sodann setzen wir Gleichung <a href="regression.html#eq:b04">(12.8)</a> in Gleichung <a href="regression.html#eq:sseb1">(12.4)</a> ein (nachdem wir durch -2 geteilt haben):</p>
<p><span class="math display" id="eq:insert1">\[\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\beta_0 \cdot x_i-\beta_1 \cdot x_i^2\right)=0
\tag{12.9}
\end{equation}\]</span>
<span class="math display" id="eq:insert2">\[\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i+\hat\beta_1 \cdot \bar{x} \cdot x_i-\hat\beta_1 \cdot x_i^2\right)=0
\tag{12.10}
\end{equation}\]</span></p>
<p>Schließlich lösen wir Gleichung <a href="regression.html#eq:insert2">(12.10)</a> nach <span class="math inline">\(\beta_1\)</span> auf:</p>
<p><span class="math display" id="eq:b11">\[\begin{equation}
\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i\right)-\hat\beta_1 \cdot \sum_{i=1}^{n}\left(x_i^2-\bar{x} \cdot x_i\right)=0
\tag{12.11}
\end{equation}\]</span>
<span class="math display" id="eq:b12">\[\begin{equation}
\hat\beta_1=\frac{\sum_{i=1}^{n}\left(x_i \cdot y_i-\bar{y} \cdot x_i\right)}{\sum_{i=1}^{n}\left(x_i^2-\bar{x} \cdot x_i\right)}
\tag{12.12}
\end{equation}\]</span></p>
<p>Über eine Reihe von Schritten, die ich hier überspringe, erhalten wir:</p>
<p><span class="math display" id="eq:b13">\[\begin{equation}
\hat\beta_1=\frac{SSXY}{SSX}
\tag{12.13}
\end{equation}\]</span></p>
<p><span class="math inline">\(SSX=\sum_{i=1}^{n}\left(x_i-\bar{x}\right)^2\)</span> ist ein Maß für die Varianz der Daten in <span class="math inline">\(x\)</span>-Richtung. <span class="math inline">\(SSXY=\sum_{i=1}^{n}\left(x_i-\bar{x}\right) \cdot \left(y_i-\bar{y}\right)\)</span> ist ein Maß für die Kovarianz der Daten. Es gibt auch <span class="math inline">\(SSY=\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2\)</span>, das entsprechend ein Maß für die Varianz der Daten in <span class="math inline">\(y\)</span>-Richtung ist. Gleichung <a href="regression.html#eq:b13">(12.13)</a> ist eine exakte Lösung für <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p>Wir setzen nun Gleichung <a href="regression.html#eq:b13">(12.13)</a> in Gleichung <a href="regression.html#eq:b04">(12.8)</a> ein und haben eine exakte Lösung für <span class="math inline">\(\hat\beta_0\)</span>. Abbildung <a href="regression.html#fig:lmxt">12.4</a> zeigt eine Grafik der so berechneten Regressionsgeraden für unsere Reisedaten.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="regression.html#cb63-1"></a><span class="co"># lineare Regression Reisedaten</span></span>
<span id="cb63-2"><a href="regression.html#cb63-2"></a>dat<span class="op">$</span>x &lt;-<span class="st"> </span>dat<span class="op">$</span>x<span class="op">/</span><span class="dv">1000</span></span>
<span id="cb63-3"><a href="regression.html#cb63-3"></a>fit3 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> t <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> dat)</span>
<span id="cb63-4"><a href="regression.html#cb63-4"></a><span class="kw">coef</span>(<span class="kw">summary</span>(fit3))</span></code></pre></div>
<pre><code>##              Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 23.547946  1.9240147 12.23896 3.088712e-21
## x            1.931042  0.1163789 16.59272 8.115580e-30</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="regression.html#cb65-1"></a><span class="co"># Modellanpassung plotten</span></span>
<span id="cb65-2"><a href="regression.html#cb65-2"></a><span class="kw">plot</span>(dat<span class="op">$</span>x, dat<span class="op">$</span>t,</span>
<span id="cb65-3"><a href="regression.html#cb65-3"></a>     <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>,</span>
<span id="cb65-4"><a href="regression.html#cb65-4"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Entfernung (km)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Reisezeit (Minuten)&quot;</span>)</span>
<span id="cb65-5"><a href="regression.html#cb65-5"></a><span class="kw">abline</span>(<span class="kw">coef</span>(fit3), <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:lmxt"></span>
<img src="eids_files/figure-html/lmxt-1.png" alt="Reisezeit in Abhängigkeit zur Entfernung: Lineare Regression $y_i=23.55+1.93\cdot x_i+\epsilon_i$" width="80%" />
<p class="caption">
Abbildung 12.4: Reisezeit in Abhängigkeit zur Entfernung: Lineare Regression <span class="math inline">\(y_i=23.55+1.93\cdot x_i+\epsilon_i\)</span>
</p>
</div>
</div>
<div id="signifikanz-der-regression" class="section level2">
<h2><span class="header-section-number">12.5</span> Signifikanz der Regression</h2>
<p>Wenn wir Werte für die Regressionsparameter haben, müssen wir uns fragen, ob diese Werte statistisch signifikant sind oder ob sie durch Zufall aus dem (angenommenen) Zufallsprozess der Stichprobenziehung entstanden sein könnten. Dazu testen wir formal, ob die vom Modell erklärte Varianz in den Daten signifikant größer als die nicht erklärte Varianz ist. Das ist ein F-Test-Problem, das wir über die sogenannte Varianzanalyse (ANOVA) angehen. ANOVA beginnt mit der Erstellung der ANOVA-Tabelle (Tabelle <a href="regression.html#tab:anova">12.3</a>). Dies geschieht oft im Hintergrund in Software wie <em>R</em> und wird eigentlich nicht so häufig explizit betrachtet.</p>
<table style="width:100%;">
<caption><span id="tab:anova">Tabelle 12.3: </span> ANOVA-Tabelle für lineare Regression.</caption>
<colgroup>
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Varianz-<br>quelle</th>
<th align="center">Quadrat-<br>summe</th>
<th align="center">Freiheits-<br>grad (<span class="math inline">\(df\)</span>)</th>
<th align="center">Varianz</th>
<th align="center">F-Statistik (<span class="math inline">\(F_s\)</span>)</th>
<th align="center">p-Wert</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Regression</td>
<td align="center"><span class="math inline">\(SSR=\\SSY-SSE\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(\frac{SSR}{df_{SSR}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{\frac{SSR}{df_{SSR}}}{s^2}\)</span></td>
<td align="center"><span class="math inline">\(1-F\left(F_s,1,n-2\right)\)</span></td>
</tr>
<tr class="even">
<td align="center">Fehler</td>
<td align="center"><span class="math inline">\(SSE\)</span></td>
<td align="center"><span class="math inline">\(n-2\)</span></td>
<td align="center"><span class="math inline">\(\frac{SSE}{df_{SSE}}=s^2\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Gesamt</td>
<td align="center"><span class="math inline">\(SSY\)</span></td>
<td align="center"><span class="math inline">\(n-1\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Schauen wir uns zunächst die zweiten Spalte von Tabelle <a href="regression.html#tab:anova">12.3</a> an: <span class="math inline">\(SSY=\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2\)</span> ist ein Maß für die Gesamtvarianz der Daten (in <span class="math inline">\(y\)</span>-Richtung), d.h. wie stark die Datenpunkte um den Gesamtmittelwert streuen (Abbildung <a href="regression.html#fig:ssysse">12.5</a>, links). <span class="math inline">\(SSE=\sum_{i=1}^{n}\left(\epsilon_i\right)^2=\sum_{i=1}^{n}\left(y_i-\left(\beta_0+\beta_1 \cdot x_i\right)\right)^2\)</span> ist ein Maß für die Fehlervarianz, d.h. wie stark die Datenpunkte um die Regressionsgerade streuen (Abbildung <a href="regression.html#fig:ssysse">12.5</a>, rechts). Dies ist die Varianz, die nach der Modellanpassung übrig ist (“nicht erklärt”). <span class="math inline">\(SSR=SSY-SSE\)</span> ist folglich ein Maß für die vom Modell erklärte Varianz.</p>
<div class="figure" style="text-align: center"><span id="fig:ssysse"></span>
<img src="figs/ssy.jpg" alt="Variation der Datenpunkte um den Mittelwert, zusammengefasst durch $SSY$ (links), und um die Regressionsgerade, zusammengefasst durch $SSE$ (rechts)." width="50%" /><img src="figs/sse.jpg" alt="Variation der Datenpunkte um den Mittelwert, zusammengefasst durch $SSY$ (links), und um die Regressionsgerade, zusammengefasst durch $SSE$ (rechts)." width="50%" />
<p class="caption">
Abbildung 12.5: Variation der Datenpunkte um den Mittelwert, zusammengefasst durch <span class="math inline">\(SSY\)</span> (links), und um die Regressionsgerade, zusammengefasst durch <span class="math inline">\(SSE\)</span> (rechts).
</p>
</div>
<p>In der dritten Spalte der Tabelle <a href="regression.html#tab:anova">12.3</a> stehen die Freiheitsgrade der drei Varianzterme, die als Anzahl der Werte in einer Stichprobe, die für die Berechnung der jeweiligen Parameter frei zur Verfügung stehen, verstanden werden können (vgl. Kapitel <a href="streuung.html#streuung">4</a>). In die Berechnung von <span class="math inline">\(SSY\)</span> geht <span class="math inline">\(\bar y\)</span> ein, fuer dessen Berechnung die Werte der Stichprobe bereits einmal verwendet wurden. Dadurch ist die Anzahl Freiheitsgrade <span class="math inline">\(df_{SSY}=n-1\)</span>. In die Berechnung von <span class="math inline">\(SSE\)</span> gehen <span class="math inline">\(\beta_0\)</span> und <span class="math inline">\(\beta_1\)</span> ein (Gleichung <a href="regression.html#eq:sse">(12.2)</a>), d.h. die Anzahl Freiheitsgrade <span class="math inline">\(df_{SSE}=n-2\)</span>. Fuer <span class="math inline">\(SSR\)</span> gilt dann einfach: <span class="math inline">\(df_{SSR}=df_{SSY}-df_{SSE}=1\)</span>. Die Freiheitsgrade werden verwendet, um die Varianzterme in der vierten Spalte der Tabelle <a href="regression.html#tab:anova">12.3</a> zu normalisieren, wobei <span class="math inline">\(s^2\)</span> Fehlervarianz genannt wird.</p>
<p>In der fünften Spalte der Tabelle <a href="regression.html#tab:anova">12.3</a> finden wir das Verhältnis von zwei Varianzen; Regressionsvarianz über Fehlervarianz. Von einer signifikanten Regression erwarten wir, dass die (durch das Modell erklärte) Regressionsvarianz viel größer ist als die (durch das Modell nicht erklärte) Fehlervarianz. Dies ist ein F-Test Problem, bei dem getestet wird, ob sich die durch das Modell <em>erklärte</em> Varianz <strong>signifikant</strong> von der durch das Modell <em>nicht erklärten</em> Varianz unterscheidet. Das Verhältnis der beiden Varianzen dient als F-Statistik (<span class="math inline">\(F_s\)</span>).</p>
<p>Die sechste Spalte der Tabelle <a href="regression.html#tab:anova">12.3</a> gibt dann den p-Wert des F-Tests an, d.h. die Wahrscheinlichkeit, <span class="math inline">\(F_s\)</span> oder einen größeren Wert (d.h. <strong>ein noch besseres Modell</strong>) zufällig zu erhalten, wenn die Nullhypothese (<span class="math inline">\(H_0\)</span>) wahr ist. <span class="math inline">\(H_0:\frac{SSR}{df_{SSR}}=s^2\)</span>, d.h. die beiden Varianzen sind gleich; <span class="math inline">\(H_1:\frac{SSR}{df_{SSR}}&gt;s^2\)</span>, d.h. die erklärte Varianz ist größer als die nicht erklärte. <span class="math inline">\(F_s\)</span> folgt einer F-Verteilung mit den Parametern <span class="math inline">\(1\)</span> und <span class="math inline">\(n-2\)</span> unter der Nullhypothese (Abbildung <a href="regression.html#fig:fcdf">12.6</a>). Die rote Linie in Abbildung <a href="regression.html#fig:fcdf">12.6</a> markiert einen bestimmten Wert von <span class="math inline">\(F_s\)</span> (zwischen 10 und 11) und den entsprechenden Wert der Verteilungsfunktion der F-Verteilung (<span class="math inline">\(F\left(F_s,1,n-2\right)\)</span>). Der p-Wert ist <span class="math inline">\(\Pr\left(Z&gt; F_s\right)=1-F\left(F_s,1,n-2\right)\)</span> und beschreibt die Wahrscheinlichkeit, dieses oder ein größeres Varianzverhältnis zufällig (aufgrund der zufälligen Stichprobenziehung) zu erhalten, selbst wenn die beiden Varianzen tatsächlich gleich sind.</p>
<div class="figure" style="text-align: center"><span id="fig:fcdf"></span>
<img src="figs/cdf_f.jpg" alt="Verteilungsfunktion der F-Verteilung der F-Statistik $F_s$. Rot: Bestimmter Wert für $F_s$ und entsprechender Wert der Verteilungsfunktion." width="80%" />
<p class="caption">
Abbildung 12.6: Verteilungsfunktion der F-Verteilung der F-Statistik <span class="math inline">\(F_s\)</span>. Rot: Bestimmter Wert für <span class="math inline">\(F_s\)</span> und entsprechender Wert der Verteilungsfunktion.
</p>
</div>
<p>Im Beispiel der Reisedaten ist der p-Wert wesentlich kleiner als das konventionelle Signifikanzniveau 0.01, daher lehnen wir die die Nullhypothese ab und bezeichnen die Regression als statistisch signifikant (Tabelle <a href="regression.html#tab:reisezeit">12.4</a>).</p>
<table style="width:100%;">
<caption><span id="tab:reisezeit">Tabelle 12.4: </span> ANOVA-Tabelle für die Regression des Beispiels Reisedaten.</caption>
<colgroup>
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Varianz-<br>quelle</th>
<th align="center">Quadrat-<br>summe</th>
<th align="center">Freiheits-<br>grad (<span class="math inline">\(df\)</span>)</th>
<th align="center">Varianz</th>
<th align="center">F-Statistik (<span class="math inline">\(F_s\)</span>)</th>
<th align="center">p-Wert</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Regression</td>
<td align="center">26253</td>
<td align="center">1</td>
<td align="center">26253</td>
<td align="center">275.32</td>
<td align="center">8.12e-30</td>
</tr>
<tr class="even">
<td align="center">Fehler</td>
<td align="center">9059</td>
<td align="center">95</td>
<td align="center">95.36</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Gesamt</td>
<td align="center">35312</td>
<td align="center">96</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
</div>
<div id="konfidenzintervalle-und-signifikanz-der-parameter" class="section level2">
<h2><span class="header-section-number">12.6</span> Konfidenzintervalle und Signifikanz der Parameter</h2>
<p>Da die Modellanpassung nicht perfekt ist, haben die Parameterschätzer Standardfehler, d.h. sie werden wie andere statistische Kennzahlen als Realisationen eines Zufallsprozesses interpretiert. Das führt uns zu Konfidenzintervallen und t-Tests auf Signifikanz der einzelnen Parameter.</p>
<p>Der <strong>Standardfehlern</strong> für <span class="math inline">\(\hat\beta_0\)</span> ist:
<span class="math display" id="eq:seb0">\[\begin{equation}
s_{\hat\beta_0}=\sqrt{\frac{\sum_{i=1}^{n}x_i^2}{n} \cdot \frac{s^2}{SSX}}
\tag{12.14}
\end{equation}\]</span></p>
<p>Wenn wir diese Formel in ihre einzelnen Teile zerlegen, sehen wir: Je mehr Datenpunkte <span class="math inline">\(n\)</span> wir haben, desto kleiner ist der Standardfehler, d.h. desto mehr Vertrauen haben wir in die Schätzung. Außerdem gilt, je größer die Abweichung in <span class="math inline">\(x\)</span> (<span class="math inline">\(SSX\)</span>), desto kleiner der Standardfehler. Beide Effekte machen intuitiv Sinn: Je mehr Datenpunkte wir haben und je mehr Ausprägungen von <span class="math inline">\(x\)</span> wir abgedeckt haben, desto sicherer können wir sein, dass wir mit unserer Stichprobe nicht viel verpasst haben. Umgekehrt gilt: Je größer die Fehlervarianz <span class="math inline">\(s^2\)</span>, d.h. je kleiner die Erklärungskraft unseres Modells, desto größer der Standardfehler. Und je mehr <span class="math inline">\(x\)</span>-Datenpunkte von Null entfernt sind, d.h. je größer <span class="math inline">\(\sum_{i=1}^{n}x_i^2\)</span>, desto geringer ist unser Vertrauen in den Achsenabschnitt (wo <span class="math inline">\(x=0\)</span>) und damit steigt der Standardfehler.</p>
<p>Der Standardfehler für <span class="math inline">\(\hat\beta_1\)</span> ist:
<span class="math display" id="eq:seb1">\[\begin{equation}
s_{\hat\beta_1}=\sqrt{\frac{s^2}{SSX}}
\tag{12.15}
\end{equation}\]</span></p>
<p>Hier gilt die gleiche Interpretation wie zuvor, außer dass es keinen Einfluss der Größe der <span class="math inline">\(x\)</span>-Datenpunkte gibt.</p>
<p>Wir können auch einen Standardfehler für neue Vorhersagen <span class="math inline">\(\hat y\)</span> für gegebene <span class="math inline">\(\hat x\)</span> festlegen:
<span class="math display" id="eq:sey">\[\begin{equation}
s_{\hat y}=\sqrt{s^2 \cdot \left(\frac{1}{n}+\frac{\left(\hat x-\bar x\right)^2}{SSX}\right)}
\tag{12.16}
\end{equation}\]</span></p>
<p>Dieselbe Interpretation gilt auch hier, nur dass jetzt ein zusätzlicher Term <span class="math inline">\(\left(\hat x-\bar x\right)^2\)</span> auftaucht, der besagt, je weiter der neue <span class="math inline">\(x\)</span>-Wert vom Zentrum der ursprünglichen Daten (den Trainings- oder Kalibrierungsdaten) entfernt ist, desto größer ist der Standardfehler der neuen Vorhersage, d.h. desto geringer ist die Zuversicht, dass sie korrekt ist.</p>
<p>Anmerkung: Die Formeln für die Standardfehler ergeben sich aus den grundlegenden Annahmen der linearen Regression, auf die wir weiter unten eingehen werden. Dies kann mathematisch hergeleitet werden, wird hier aber ausgelassen.</p>
<p>Aus den Standardfehlern können wir <strong>Konfidenzintervalle</strong> für die Parameterschätzer wie folgt berechnen:
<span class="math display" id="eq:cib01">\[\begin{equation}
\Pr\left(\hat\beta_0-t_{n-2;0.975} \cdot s_{\hat\beta_0}\leq \beta_0\leq \hat\beta_0+t_{n-2;0.975} \cdot s_{\hat\beta_0}\right)=0.95
\tag{12.17}
\end{equation}\]</span></p>
<p>Das Symbol <span class="math inline">\(\Pr(\cdot)\)</span> bedeutet Wahrscheinlichkeit. Das Symbol <span class="math inline">\(t_{n-2;0.975}\)</span> steht für das 0.975-Perzentil der t-Verteilung mit <span class="math inline">\(n-2\)</span> Freiheitsgraden. Die Gleichung <a href="regression.html#eq:cib01">(12.17)</a> ist das zentrale 95%-Konfidenzintervall, d.h. ein Intervall, in dem der wahre Parameterwert, hier <span class="math inline">\(\beta_0\)</span>, mit einer Wahrscheinlichkeit von 0.95 liegt. Vgl. Konfidenzintervall des Mittelwertschätzers (Kapitel <a href="schaetzen.html#schaetzen">8</a>).</p>
<p>Wir können das Intervall auch wie folgt schreiben:
<span class="math display" id="eq:cib02">\[\begin{equation}
KI=\left[\hat\beta_0-t_{n-2;0.975} \cdot s_{\hat\beta_0};\hat\beta_0+t_{n-2;0.975} \cdot s_{\hat\beta_0}\right]
\tag{12.18}
\end{equation}\]</span></p>
<p>Wie zu sehen ist, liegt das Konfidenzintervall symmetrisch um den Parameterschätzwert <span class="math inline">\(\hat\beta_0\)</span> und ergibt sich aus einer t-Verteilung mit dem Parameter <span class="math inline">\(n-2\)</span>, deren Breite durch den Standardfehler <span class="math inline">\(s_{\hat\beta_0}\)</span> moduliert wird. Beachten Sie, dass die Breite der t-Verteilung ebenfalls durch den Stichprobenumfang kontrolliert wird und mit zunehmendem <span class="math inline">\(n\)</span> immer schmaler wird.</p>
<p>Die gleichen Formeln gelten für <span class="math inline">\(\beta_1\)</span> und <span class="math inline">\(y\)</span>:
<span class="math display" id="eq:cib1">\[\begin{equation}
\Pr\left(\hat\beta_1-t_{n-2;0.975} \cdot s_{\hat\beta_1}\leq \beta_1\leq \hat\beta_1+t_{n-2;0.975} \cdot s_{\hat\beta_1}\right)=0.95
\tag{12.19}
\end{equation}\]</span>
<span class="math display" id="eq:ciy">\[\begin{equation}
\Pr\left(\hat y-t_{n-2;0.975} \cdot s_{\hat y}\leq y\leq \hat y+t_{n-2;0.975} \cdot s_{\hat y}\right)=0.95
\tag{12.20}
\end{equation}\]</span></p>
<p>Die Formeln für die Konfidenzintervalle (Gleichungen <a href="regression.html#eq:cib01">(12.17)</a>, <a href="regression.html#eq:cib1">(12.19)</a> und <a href="regression.html#eq:ciy">(12.20)</a>) ergeben sich aus den Grundannahmen der linearen Regression (vgl. Kapitel <a href="schaetzen.html#schaetzen">8</a>): Die Residuen sind <em>unabhängig identisch verteilt (u.i.v.)</em> gemäß einer <em>Normalverteilung</em>, d.h. <span class="math inline">\(\epsilon_i\sim N(0,\sigma)\)</span>, und <em>das lineare Modell ist korrekt</em>. Dann lässt sich mathematisch zeigen, dass <span class="math inline">\(\frac{\hat\beta_0-\beta_0}{s_{\hat\beta_0}}\)</span>, <span class="math inline">\(\frac{\hat\beta_1-\beta_1}{s_{\hat\beta_1}}\)</span> und <span class="math inline">\(\frac{\hat y-y}{s_{\hat y}}\)</span> <span class="math inline">\(t_{n-2}\)</span>-verteilt sind (t-Verteilung mit <span class="math inline">\(n-2\)</span> Freiheitsgraden).</p>
<p>Da das zentrale 95%-Konfidenzintervall einer <span class="math inline">\(t_{n-2}\)</span>-verteilten Zufallsvariablen <span class="math inline">\(Z\)</span> <span class="math inline">\(\Pr\left(-t_{n-2;0.975}\leq Z\leq t_{n-2;0. 975}\right)=0.95\)</span> ist (Abbildung <a href="regression.html#fig:tpdfcdf">12.7</a>), können wir jeden der oben genannten drei Terme für <span class="math inline">\(Z\)</span> einsetzen und neu anordnen, um zu den Gleichungen <a href="regression.html#eq:cib01">(12.17)</a>, <a href="regression.html#eq:cib1">(12.19)</a> und <a href="regression.html#eq:ciy">(12.20)</a> zu gelangen.</p>
<div class="figure" style="text-align: center"><span id="fig:tpdfcdf"></span>
<img src="figs/pdf_t.jpg" alt="Links: Dichtefunktion einer t-verteilten Zufallsvariablen $Z$, wobei das zentrale 95%-Konfidenzintervall rot markiert ist. 95% der Dichtefunktion liegen zwischen den beiden Grenzen, 2.5% liegen links von der unteren Grenze und 2.5% rechts von der oberen Grenze. Rechts: Verteilungsfunktion der gleichen t-verteilten Zufallsvariablen $Z$. Die obere Grenze des 95%-Konfidenzintervalls ist als $t_{n-2;0.975}$ definiert, d.h. das 0.975-Perzentil der Verteilung, während die untere Grenze als $t_{n-2;0.025}$ definiert ist, was aufgrund der Symmetrie der Verteilung $-t_{n-2;0.975}$ entspricht." width="50%" /><img src="figs/cdf_t.jpg" alt="Links: Dichtefunktion einer t-verteilten Zufallsvariablen $Z$, wobei das zentrale 95%-Konfidenzintervall rot markiert ist. 95% der Dichtefunktion liegen zwischen den beiden Grenzen, 2.5% liegen links von der unteren Grenze und 2.5% rechts von der oberen Grenze. Rechts: Verteilungsfunktion der gleichen t-verteilten Zufallsvariablen $Z$. Die obere Grenze des 95%-Konfidenzintervalls ist als $t_{n-2;0.975}$ definiert, d.h. das 0.975-Perzentil der Verteilung, während die untere Grenze als $t_{n-2;0.025}$ definiert ist, was aufgrund der Symmetrie der Verteilung $-t_{n-2;0.975}$ entspricht." width="50%" />
<p class="caption">
Abbildung 12.7: Links: Dichtefunktion einer t-verteilten Zufallsvariablen <span class="math inline">\(Z\)</span>, wobei das zentrale 95%-Konfidenzintervall rot markiert ist. 95% der Dichtefunktion liegen zwischen den beiden Grenzen, 2.5% liegen links von der unteren Grenze und 2.5% rechts von der oberen Grenze. Rechts: Verteilungsfunktion der gleichen t-verteilten Zufallsvariablen <span class="math inline">\(Z\)</span>. Die obere Grenze des 95%-Konfidenzintervalls ist als <span class="math inline">\(t_{n-2;0.975}\)</span> definiert, d.h. das 0.975-Perzentil der Verteilung, während die untere Grenze als <span class="math inline">\(t_{n-2;0.025}\)</span> definiert ist, was aufgrund der Symmetrie der Verteilung <span class="math inline">\(-t_{n-2;0.975}\)</span> entspricht.
</p>
</div>
<p>Die Signifikanz der Parameterschätzer wird mit Hilfe eines t-Tests ermittelt (vgl. Kapitel <a href="ttest.html#ttest">9</a> und <a href="ftest.html#ftest">10</a>).</p>
<p>Die <strong>Nullhypothese</strong> ist, dass die wahren Parameterwerte gleich Null sind, d.h. die Parameterschätzer <em>nicht</em> signifikant sind:
<span class="math display" id="eq:h0b0">\[\begin{equation}
H_0:\beta_0=0
\tag{12.21}
\end{equation}\]</span>
<span class="math display" id="eq:h0b1">\[\begin{equation}
H_0:\beta_1=0
\tag{12.22}
\end{equation}\]</span></p>
<p>Diese Hypothese wird gegen die <strong>Alternativhypothese</strong> getestet, dass die wahren Parameterwerte <em>un</em>gleich Null sind, d.h. dass die Parameterschätzer signifikant sind:
<span class="math display" id="eq:h1b0">\[\begin{equation}
H_1:\beta_0\neq 0
\tag{12.23}
\end{equation}\]</span>
<span class="math display" id="eq:h1b1">\[\begin{equation}
H_1:\beta_1\neq 0
\tag{12.24}
\end{equation}\]</span></p>
<p>Die Teststatistiken sind:</p>
<p><span class="math display" id="eq:tsb0">\[\begin{equation}
t_s=\frac{\hat\beta_0-0}{s_{\hat\beta_0}}\sim t_{n-2}
\tag{12.25}
\end{equation}\]</span>
<span class="math display" id="eq:tsb1">\[\begin{equation}
t_s=\frac{\hat\beta_1-0}{s_{\hat\beta_1}}\sim t_{n-2}
\tag{12.26}
\end{equation}\]</span></p>
<p>Das “Tilde”-Symbol (<span class="math inline">\(\sim\)</span>) bedeutet, dass die Teststatistik einer bestimmten Verteilung folgt, hier der t-Verteilung. Dies ergibt sich wiederum aus den oben erwähnten Regressionsannahmen. Die Annahmen sind die gleichen wie beim üblichen t-Test der Mittelwerte (Kapitel <a href="ttest.html#ttest">9</a> und <a href="ftest.html#ftest">10</a>), außer dass im Fall der linearen Regression die Residuen als u.i.v. normal angenommen werden, während im Fall der Mittelwerte die tatsächlichen Datenpunkte <span class="math inline">\(y\)</span> als u.i.v. normal angenommen werden.</p>
<p>Analog zum üblichen 2-seitigen t-Test ist der p-Wert definiert als:
<span class="math display" id="eq:pv">\[\begin{equation}
2 \cdot \Pr\left(t&gt;|t_s|\right)=2 \cdot \left(1-F_t\left(|t_s|\right)\right)
\tag{12.27}
\end{equation}\]</span></p>
<p>Das Symbol <span class="math inline">\(F_t\left(|t_s|\right)\)</span> bezeichnet den Wert der kumulativen Verteilungsfunktion der t-Verteilung an der Stelle des absoluten Wertes der Teststatistik (<span class="math inline">\(|t_s|\)</span>, Abbildung <a href="regression.html#fig:tc">12.8</a>). Mit einem Signifikanzniveau von z.B. <span class="math inline">\(\alpha=0.01\)</span> gelangen wir zu einem kritischen Wert der Teststatistik <span class="math inline">\(t_c=t_{n-2;0.995}\)</span>, bei dessen Überschreitung wir die Nullhypothese ablehnen und die Parameterschätzer als signifikant bezeichnen (Abbildung <a href="regression.html#fig:tc">12.8</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:tc"></span>
<img src="figs/tc.jpg" alt="Schema des t-Tests auf Signifikanz dern Parameterschätzer. Die Teststatistik folgt einer t-Verteilung unter der Nullhypothese. Der tatsächliche Wert der Teststatistik $t_s$ ist blau markiert und wird für den 2-seitigen Test bei Null gespiegelt. Der kritische Wert der Teststatistik $t_c$, den wir von einem Signifikanzniveau von $\alpha=0.01$ erhalten, ist rot markiert; auch dieser wird für den 2-seitigen Test gespiegelt. Wir lehnen die Nullhypothese ab, wenn $|t_s|&gt;t_c$, d.h. für Werte von $t_s$ kleiner als $-t_c$ und größer als $t_c$, und nennen diese Parameterschätzer dann signifikant. Wir behalten die Nullhypothese bei wenn $|t_s|\leq t_c$, d.h. für Werte von $t_s$ zwischen $-t_c$ und $t_c$, und nennen diesen Parameterschätzer dann (vorläufig) nicht signifikant. In dem gezeigten Beispiel ist der Parameterschätzer nicht signifikant." width="80%" />
<p class="caption">
Abbildung 12.8: Schema des t-Tests auf Signifikanz dern Parameterschätzer. Die Teststatistik folgt einer t-Verteilung unter der Nullhypothese. Der tatsächliche Wert der Teststatistik <span class="math inline">\(t_s\)</span> ist blau markiert und wird für den 2-seitigen Test bei Null gespiegelt. Der kritische Wert der Teststatistik <span class="math inline">\(t_c\)</span>, den wir von einem Signifikanzniveau von <span class="math inline">\(\alpha=0.01\)</span> erhalten, ist rot markiert; auch dieser wird für den 2-seitigen Test gespiegelt. Wir lehnen die Nullhypothese ab, wenn <span class="math inline">\(|t_s|&gt;t_c\)</span>, d.h. für Werte von <span class="math inline">\(t_s\)</span> kleiner als <span class="math inline">\(-t_c\)</span> und größer als <span class="math inline">\(t_c\)</span>, und nennen diese Parameterschätzer dann signifikant. Wir behalten die Nullhypothese bei wenn <span class="math inline">\(|t_s|\leq t_c\)</span>, d.h. für Werte von <span class="math inline">\(t_s\)</span> zwischen <span class="math inline">\(-t_c\)</span> und <span class="math inline">\(t_c\)</span>, und nennen diesen Parameterschätzer dann (vorläufig) nicht signifikant. In dem gezeigten Beispiel ist der Parameterschätzer nicht signifikant.
</p>
</div>
</div>
<div id="güte-der-modellanpassung" class="section level2">
<h2><span class="header-section-number">12.7</span> Güte der Modellanpassung</h2>
<p>Die Signifikanz der Parameter der Regression ist eine Sache. Wie gut aber ist das Modell im Beschreiben der Daten? D.h. wieviel von der Varianz in den Daten wird vom Modell erklärt?</p>
<p>Die Beurteilung der Güte der Modellanpassung kann in erster Linie durch das <strong>Bestimmtheitsmaß</strong> (<span class="math inline">\(r^2\)</span>) erfolgen, welches als der Anteil der Varianz (in <span class="math inline">\(y\)</span>-Richtung) definiert ist, der durch das Modell erklärt wird. Das Bestimmtheitsmaß ist der Korrelationskoeffizient nach Bravais-Pearson zum Quadrat (vgl. Kapitel <a href="korrelation.html#korrelation">5</a>).</p>
<p><span class="math display" id="eq:r2">\[\begin{equation}
r^2=\frac{SSY-SSE}{SSY}=1-\frac{SSE}{SSY}
\tag{12.28}
\end{equation}\]</span></p>
<p>Wie wir sehen, wenn das Modell nicht mehr Variation als die Gesamtvariation um den Mittelwert erklärt, d.h. <span class="math inline">\(SSE=SSY\)</span>, dann <span class="math inline">\(r^2=0\)</span>. Umgekehrt, wenn das Modell perfekt zu den Daten passt, d.h. <span class="math inline">\(SSE=0\)</span>, dann ist <span class="math inline">\(r^2=1\)</span>. Werte dazwischen stellen unterschiedliche Grade der Anpassungsgüte dar. Dies kann wiederum mit Abbildung <a href="regression.html#fig:ssysse">12.5</a> veranschaulicht werden, wobei der linke Teil <span class="math inline">\(SSY\)</span> und der rechte Teil <span class="math inline">\(SSE\)</span> verdeutlicht.</p>
<p>Wenn es darum geht, Modelle unterschiedlicher Komplexität (d.h. mit mehr oder weniger Parametern) mit <span class="math inline">\(r^2\)</span> zu vergleichen, dann ist es sinnvoll, die Metrik mit der Anzahl der Modellparameter zu korrigieren, da komplexere Modelle (mehr Parameter) automatisch zu besseren Anpassungen führen, einfach aufgrund der größeren Freiheitsgrade, die komplexere Modelle bei der Anpassung der Daten haben. Dies führt zum <strong>korregierten <span class="math inline">\(r^2\)</span></strong>:</p>
<p><span class="math display" id="eq:adjr2">\[\begin{equation}
\bar r^2=1-\frac{\frac{SSE}{df_{SSE}}}{\frac{SSY}{df_{SSY}}}=1-\frac{SSE}{SSY} \cdot \frac{df_{SSY}}{df_{SSE}}
\tag{12.29}
\end{equation}\]</span></p>
<p><Beispiel></p>
<p>Was heißt Güte der Modellanpassung? Sind alle Modellannahmen erfüllt? Folgende Annahmen ergeben sich aus der <strong>Maximum-Likelihood-Theorie</strong> (vgl. Schätzen von Verteilungsparametern, Kapitel <a href="schaetzen.html#schaetzen">8</a>):</p>
<ul>
<li>Die Residuen sind <strong>unabhängig</strong>, in diesem Fall gibt es keine serielle Korrelation in der Residuengraphik - dies kann mit dem Durbin-Watson-Test getestet werden</li>
<li>Die Residuen sind <strong>normalverteilt</strong> - dies kann visuell mit Hilfe des Quantil-Quantil-Diagramms (QQ-Plot) und dem Residuen-Histogramm beurteilt werden, und kann mit dem Kolmogorov-Smirnov-Test (Kapitel <a href="chi2testkstest.html#chi2testkstest">11</a>) und dem Shapiro-Wilk-Test getestet werden</li>
<li>Die Varianz ist für alle Residuen konstant (die Residuen sind <strong>homoskedastisch</strong>), d.h. es erfolgt kein “Auffächern” der Residuen</li>
</ul>
<p>Sind diese Annahmen nicht erfüllt, können wir auf Datentransformation, gewichtete Regression oder Generalisierte Lineare Modelle zurückgreifen. Letzteres ist die bevorzugte Option und wird im Master <em>Global Change Geography</em> unterrichtet.</p>
<p>Zu unserem Beispiel: Eine erste nützliche diagnostische Darstellung ist die der Residuen in Serie, d.h. nach Index <span class="math inline">\(i\)</span>, um zu sehen, ob es ein Muster aufgrund des Datenerfassungsprozesses gibt (Abbildung <a href="regression.html#fig:abb12-4">12.9</a>). Dieser Datensatz zeigt kein erkennbares Muster an, was für eine Unabhängigkeit der Residuen spricht.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="regression.html#cb66-1"></a><span class="co"># Residuen gegen Index plotten</span></span>
<span id="cb66-2"><a href="regression.html#cb66-2"></a><span class="kw">plot</span>(<span class="kw">residuals</span>(fit3), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb66-3"><a href="regression.html#cb66-3"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:abb12-4"></span>
<img src="eids_files/figure-html/abb12-4-1.png" alt="Lineare Regression der Reisedaten. Residuen in Serie, d.h. nach Index $i$." width="80%" />
<p class="caption">
Abbildung 12.9: Lineare Regression der Reisedaten. Residuen in Serie, d.h. nach Index <span class="math inline">\(i\)</span>.
</p>
</div>
<p>Wir sollten auch die Residuen nach dem vorhergesagten Wert von <span class="math inline">\(y\)</span> plotten, um zu sehen, ob es ein Muster als Funktion der Größenordnung von <span class="math inline">\(y\)</span> gibt (Abbildung <a href="regression.html#fig:abb12-5">12.10</a>). Die Graphik deutet auf eine Abnahme der Residuenvarianz von links nach rechts hin, d.h. Heteroskedastizität.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="regression.html#cb67-1"></a><span class="co"># Residuen gegen vorhergesagte Werte von y plotten</span></span>
<span id="cb67-2"><a href="regression.html#cb67-2"></a><span class="kw">plot</span>(<span class="kw">fitted.values</span>(fit3),<span class="kw">residuals</span>(fit3), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;p&#39;</span>)</span>
<span id="cb67-3"><a href="regression.html#cb67-3"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:abb12-5"></span>
<img src="eids_files/figure-html/abb12-5-1.png" alt="Lineare Regression der Reisedaten. Residuen geordnet nach vorausgesagten Werten von $y$." width="80%" />
<p class="caption">
Abbildung 12.10: Lineare Regression der Reisedaten. Residuen geordnet nach vorausgesagten Werten von <span class="math inline">\(y\)</span>.
</p>
</div>
<p>Die Annahme, dass die Residuen normalverteilt sind, kann anhand des QQ-Plots beurteilt werden (Abbildung <a href="regression.html#fig:abb12-6">12.11</a>).</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="regression.html#cb68-1"></a><span class="kw">qqnorm</span>(<span class="kw">residuals</span>(fit3))</span>
<span id="cb68-2"><a href="regression.html#cb68-2"></a><span class="kw">qqline</span>(<span class="kw">residuals</span>(fit3))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:abb12-6"></span>
<img src="eids_files/figure-html/abb12-6-1.png" alt="Lineare Regression der Reisedaten. Quantil-Quantil-Diagramm (QQ-Plot) der Residuen." width="80%" />
<p class="caption">
Abbildung 12.11: Lineare Regression der Reisedaten. Quantil-Quantil-Diagramm (QQ-Plot) der Residuen.
</p>
</div>
Im QQ-Plot stellt jeder Datenpunkt ein bestimmtes Quantil der empirischen Verteilung dar. Dieses Quantil (nach Standardisierung) wird gegen den Wert dieses Quantils geplotted (vertikale Achse), der unter einer Standard-Normalverteilung (horizontale Achse) erwartet wird. Die resultierenden Formen sagen etwas über die Verteilung der Residuen aus (Abbildung <a href="regression.html#fig:qq">12.12</a>). Im Falle einer Normalverteilung zum Beispiel, fallen alle Residuen auf eine gerade Linie. In unserem Fall deutet der QQ-Plot eine leichte Rechts-Schiefe der Residuen an.
<div class="figure" style="text-align: center"><span id="fig:qq"></span>
<img src="figs/qq.gif" alt="Charakteristische Formen des QQ-Plots und was sie für die Residuen bedeuten. Quelle: https://condor.depaul.edu/sjost/it223/documents/normal-plot.htm." width="80%" />
<p class="caption">
Abbildung 12.12: Charakteristische Formen des QQ-Plots und was sie für die Residuen bedeuten. Quelle: <a href="https://condor.depaul.edu/sjost/it223/documents/normal-plot.htm" class="uri">https://condor.depaul.edu/sjost/it223/documents/normal-plot.htm</a>.
</p>
</div>
<p>Das Histogramm der Residuen deutet ebenfalls auf eine leichte Rechts-Schiefe hin (Abbildung <a href="regression.html#fig:abb12-7">12.13</a>).</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="regression.html#cb69-1"></a><span class="co"># Histogramm der Residuen plotten</span></span>
<span id="cb69-2"><a href="regression.html#cb69-2"></a><span class="kw">hist</span>(<span class="kw">residuals</span>(fit3))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:abb12-7"></span>
<img src="eids_files/figure-html/abb12-7-1.png" alt="ReisezeitLineare Regression der Reisedaten. Histogramm der Residuen." width="80%" />
<p class="caption">
Abbildung 12.13: ReisezeitLineare Regression der Reisedaten. Histogramm der Residuen.
</p>
</div>

</div>
</div>
<h3>Literatur</h3>
<div id="refs" class="references">
<div id="ref-wainer2009">
<p>Wainer, H. 2009. <em>Picturing the Uncertain World</em>. Book. Princeton: Princeton University Press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chi2testkstest.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="literatur.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
